{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8AMZcSSjj5Q0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeA_kbBOIqcW",
        "outputId": "b3aff7c1-85b0-44c9-8ff5-de42241f1aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q9sTTfS3j5Q2"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    seed = 42\n",
        "    device = \"cuda:0\"    \n",
        "        \n",
        "    lr = 1e-3\n",
        "    epochs = 25\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    train_5_folds = True\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "seed_everything(config.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79df124-92b5-42f9-dec0-4b60e2341f45",
        "id": "6FrCCMqQj5Q2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FKlPG5eXr8_T"
      },
      "outputs": [],
      "source": [
        "gray  = transforms.Compose([transforms.ToTensor(),\n",
        "                            transforms.RandomHorizontalFlip(p=0.5),\n",
        "                            transforms.RandomGrayscale(p=0.5)])\n",
        "\n",
        "\n",
        "flip = transforms.Compose([transforms.ToTensor(),\n",
        "           transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0fyNMCFGr8_T"
      },
      "outputs": [],
      "source": [
        "trainset1 = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=False,\n",
        "                                        transform=gray)\n",
        "\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=False,\n",
        "                                        transform=flip)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=False,\n",
        "                                        transform=flip)\n",
        "\n",
        "trainset = trainset1+ trainset2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aoTTPq62j5Q4"
      },
      "outputs": [],
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/images/Statistical_Deep_Image\",\n",
        "                                           transform = flip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iPC8oo7rj5Q4"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 64,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 64,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleNeck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        inner_channels = 4 * growth_rate\n",
        "\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(inner_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=10, init_weights=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.growth_rate = growth_rate\n",
        "        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, inner_channels, 5, stride=1, padding=1),\n",
        "            nn.MaxPool2d(3, 2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for i in range(len(nblocks)-1):\n",
        "            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n",
        "            inner_channels += growth_rate * nblocks[i]\n",
        "            out_channels = int(reduction * inner_channels)\n",
        "            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels \n",
        "        \n",
        "        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('relu', nn.ReLU())\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.linear = nn.Linear(inner_channels, num_classes)\n",
        "\n",
        "        # weight initialization\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.features(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def _make_dense_block(self, nblock, inner_channels):\n",
        "        dense_block = nn.Sequential()\n",
        "        for i in range(nblock):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n",
        "            inner_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def DenseNet_121():\n",
        "    return DenseNet([6, 12, 24, 6])"
      ],
      "metadata": {
        "id": "pX-Oy2buh-dO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = DenseNet_121().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "Os2IBshCP2bO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "1Q49TU95yZz5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(20):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "    if i %1500 ==1499:\n",
        "      with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for k, data1 in enumerate(vaild_loader, 0):\n",
        "          val_inputs, val_label = data1[0].to(device), data1[1].to(device)\n",
        "          val_output = net(val_inputs)\n",
        "          v_loss = criterion(val_output, val_label)\n",
        "          val_loss += v_loss\n",
        "          _, predicted = torch.max(val_output.data,1)\n",
        "          total += val_label.size(0)\n",
        "          correct += (predicted == val_label).sum().item()\n",
        "      print(\"validation loss {}\".format(val_loss))\n",
        "      print(\"vaildset Accuracy  : {}\".format(100* correct/total))\n",
        "      total=0\n",
        "      correct=0\n",
        "      with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs= net(images)\n",
        "          _, predicted = torch.max(outputs.data,1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print(\"testset Accuracy  : {}\".format(100* correct/total))\n",
        "      total=0\n",
        "      correct=0\n"
      ],
      "metadata": {
        "id": "q8IufYT0P4Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587ed3ba-c696-4376-d8f3-4a8a3da1c140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.6380273588895797\n",
            "Epoch: 1,Batch : 1500, Loss:0.5166021029651165\n",
            "validation loss 197.1248779296875\n",
            "vaildset Accuracy  : 54.88\n",
            "testset Accuracy  : 12.05\n",
            "Epoch: 2,Batch : 750, Loss:0.41509501087665557\n",
            "Epoch: 2,Batch : 1500, Loss:0.37374366891384125\n",
            "validation loss 151.49432373046875\n",
            "vaildset Accuracy  : 66.64\n",
            "testset Accuracy  : 13.8\n",
            "Epoch: 3,Batch : 750, Loss:0.3175129534751177\n",
            "Epoch: 3,Batch : 1500, Loss:0.2982202542126179\n",
            "validation loss 128.0334930419922\n",
            "vaildset Accuracy  : 71.89\n",
            "testset Accuracy  : 14.7\n",
            "Epoch: 4,Batch : 750, Loss:0.2585112541168928\n",
            "Epoch: 4,Batch : 1500, Loss:0.24714560756087303\n",
            "validation loss 111.3541488647461\n",
            "vaildset Accuracy  : 75.27\n",
            "testset Accuracy  : 15.0\n",
            "Epoch: 5,Batch : 750, Loss:0.21705892699211835\n",
            "Epoch: 5,Batch : 1500, Loss:0.2135029561817646\n",
            "validation loss 110.7131118774414\n",
            "vaildset Accuracy  : 76.02\n",
            "testset Accuracy  : 16.1\n",
            "Epoch: 6,Batch : 750, Loss:0.187097302056849\n",
            "Epoch: 6,Batch : 1500, Loss:0.18828031746298074\n",
            "validation loss 105.32685089111328\n",
            "vaildset Accuracy  : 77.29\n",
            "testset Accuracy  : 16.3\n",
            "Epoch: 7,Batch : 750, Loss:0.1615280497968197\n",
            "Epoch: 7,Batch : 1500, Loss:0.16455160120129586\n",
            "validation loss 104.13947296142578\n",
            "vaildset Accuracy  : 78.59\n",
            "testset Accuracy  : 16.0\n",
            "Epoch: 8,Batch : 750, Loss:0.14276022982224823\n",
            "Epoch: 8,Batch : 1500, Loss:0.1449292606599629\n",
            "validation loss 106.41246032714844\n",
            "vaildset Accuracy  : 78.44\n",
            "testset Accuracy  : 17.8\n",
            "Epoch: 9,Batch : 750, Loss:0.1246598834618926\n",
            "Epoch: 9,Batch : 1500, Loss:0.1302043989263475\n",
            "validation loss 111.89246368408203\n",
            "vaildset Accuracy  : 77.54\n",
            "testset Accuracy  : 16.9\n",
            "Epoch: 10,Batch : 750, Loss:0.11253360887430608\n",
            "Epoch: 10,Batch : 1500, Loss:0.11758825612813234\n",
            "validation loss 116.20198059082031\n",
            "vaildset Accuracy  : 78.05\n",
            "testset Accuracy  : 16.8\n",
            "Epoch: 11,Batch : 750, Loss:0.09966835743933916\n",
            "Epoch: 11,Batch : 1500, Loss:0.10441152639128268\n",
            "validation loss 121.18585205078125\n",
            "vaildset Accuracy  : 77.26\n",
            "testset Accuracy  : 17.9\n",
            "Epoch: 12,Batch : 750, Loss:0.08996121396496892\n",
            "Epoch: 12,Batch : 1500, Loss:0.09756507595255971\n",
            "validation loss 119.1899642944336\n",
            "vaildset Accuracy  : 78.47\n",
            "testset Accuracy  : 18.75\n",
            "Epoch: 13,Batch : 750, Loss:0.08089814342185855\n",
            "Epoch: 13,Batch : 1500, Loss:0.08891819685697555\n",
            "validation loss 123.73271179199219\n",
            "vaildset Accuracy  : 78.16\n",
            "testset Accuracy  : 18.85\n",
            "Epoch: 14,Batch : 750, Loss:0.07700383496657014\n",
            "Epoch: 14,Batch : 1500, Loss:0.07991605000942946\n",
            "validation loss 127.63807678222656\n",
            "vaildset Accuracy  : 77.94\n",
            "testset Accuracy  : 18.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "KqDJoCltxkKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/share/trained_model/desnse.pth'\n",
        "\n"
      ],
      "metadata": {
        "id": "S0fKwJo7P4iQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "9VJnnqHYI-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane','automobile','bird','cat','deer',\n",
        "           'dog','frog','horse','ship','truck')"
      ],
      "metadata": {
        "id": "47ZGkY0NX5if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    c= (predicted == labels).squeeze()\n",
        "    for i in range(4):\n",
        "      label= labels[i]\n",
        "      class_correct[label]+= c[i].item()\n",
        "      class_total[label]+= 1\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Accuracy of {} : {} %\".format(classes[i],100* class_correct[i]/class_total[i]))"
      ],
      "metadata": {
        "id": "QOaHVH52X5kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLanzQjcfLzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"trainset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "44HdYYfb5Eja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "xKUJChxdX5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images3, labels3 = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images3)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels3.size(0)\n",
        "    correct += (predicted == labels3).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "O8NlBkmgX5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0777cd59-dbac-48fe-a64d-2836ed528195"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset Accuracy  : 20.95\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}