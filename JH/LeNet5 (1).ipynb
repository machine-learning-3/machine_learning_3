{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJqq2BV4Uhk2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac2gOdaA3qyd",
        "outputId": "2676f9c0-11b3-44e6-f20b-bc40e2af4b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vyQQ0rYqwp9"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    seed = 42\n",
        "    device = \"cuda:0\"    \n",
        "        \n",
        "    lr = 1e-3\n",
        "    epochs = 25\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    train_5_folds = True\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "seed_everything(config.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnZf8_DlJRxC",
        "outputId": "6652bb59-a7e6-48cb-8054-d5d12d158355"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qw6bRoGLEqk"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "transform_validation = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "transform  = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3YUvobnJimS",
        "outputId": "9666cf02-d078-402b-e0c3-b6aac922293a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd8-9_oDSQK_"
      },
      "outputs": [],
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/images/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRxWLJjnKUIt"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 64,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTgHG91xbSq"
      },
      "source": [
        "모델 넣기 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkV4EfO4VplZ",
        "outputId": "3ff28048-2a95-40ca-fb77-b06dc0b16a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 14.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.10.1 timm-0.6.11\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['adv_inception_v3',\n",
              " 'bat_resnext26ts',\n",
              " 'beit_base_patch16_224',\n",
              " 'beit_base_patch16_224_in22k',\n",
              " 'beit_base_patch16_384',\n",
              " 'beit_large_patch16_224',\n",
              " 'beit_large_patch16_224_in22k',\n",
              " 'beit_large_patch16_384',\n",
              " 'beit_large_patch16_512',\n",
              " 'beitv2_base_patch16_224',\n",
              " 'beitv2_base_patch16_224_in22k',\n",
              " 'beitv2_large_patch16_224',\n",
              " 'beitv2_large_patch16_224_in22k',\n",
              " 'botnet26t_256',\n",
              " 'botnet50ts_256',\n",
              " 'cait_m36_384',\n",
              " 'cait_m48_448',\n",
              " 'cait_s24_224',\n",
              " 'cait_s24_384',\n",
              " 'cait_s36_384',\n",
              " 'cait_xs24_384',\n",
              " 'cait_xxs24_224',\n",
              " 'cait_xxs24_384',\n",
              " 'cait_xxs36_224',\n",
              " 'cait_xxs36_384',\n",
              " 'coat_lite_mini',\n",
              " 'coat_lite_small',\n",
              " 'coat_lite_tiny',\n",
              " 'coat_mini',\n",
              " 'coat_tiny',\n",
              " 'coatnet_0_224',\n",
              " 'coatnet_0_rw_224',\n",
              " 'coatnet_1_224',\n",
              " 'coatnet_1_rw_224',\n",
              " 'coatnet_2_224',\n",
              " 'coatnet_2_rw_224',\n",
              " 'coatnet_3_224',\n",
              " 'coatnet_3_rw_224',\n",
              " 'coatnet_4_224',\n",
              " 'coatnet_5_224',\n",
              " 'coatnet_bn_0_rw_224',\n",
              " 'coatnet_nano_cc_224',\n",
              " 'coatnet_nano_rw_224',\n",
              " 'coatnet_pico_rw_224',\n",
              " 'coatnet_rmlp_0_rw_224',\n",
              " 'coatnet_rmlp_1_rw_224',\n",
              " 'coatnet_rmlp_2_rw_224',\n",
              " 'coatnet_rmlp_3_rw_224',\n",
              " 'coatnet_rmlp_nano_rw_224',\n",
              " 'coatnext_nano_rw_224',\n",
              " 'convit_base',\n",
              " 'convit_small',\n",
              " 'convit_tiny',\n",
              " 'convmixer_768_32',\n",
              " 'convmixer_1024_20_ks9_p14',\n",
              " 'convmixer_1536_20',\n",
              " 'convnext_atto',\n",
              " 'convnext_atto_ols',\n",
              " 'convnext_base',\n",
              " 'convnext_base_384_in22ft1k',\n",
              " 'convnext_base_in22ft1k',\n",
              " 'convnext_base_in22k',\n",
              " 'convnext_femto',\n",
              " 'convnext_femto_ols',\n",
              " 'convnext_large',\n",
              " 'convnext_large_384_in22ft1k',\n",
              " 'convnext_large_in22ft1k',\n",
              " 'convnext_large_in22k',\n",
              " 'convnext_nano',\n",
              " 'convnext_nano_ols',\n",
              " 'convnext_pico',\n",
              " 'convnext_pico_ols',\n",
              " 'convnext_small',\n",
              " 'convnext_small_384_in22ft1k',\n",
              " 'convnext_small_in22ft1k',\n",
              " 'convnext_small_in22k',\n",
              " 'convnext_tiny',\n",
              " 'convnext_tiny_384_in22ft1k',\n",
              " 'convnext_tiny_hnf',\n",
              " 'convnext_tiny_in22ft1k',\n",
              " 'convnext_tiny_in22k',\n",
              " 'convnext_xlarge_384_in22ft1k',\n",
              " 'convnext_xlarge_in22ft1k',\n",
              " 'convnext_xlarge_in22k',\n",
              " 'crossvit_9_240',\n",
              " 'crossvit_9_dagger_240',\n",
              " 'crossvit_15_240',\n",
              " 'crossvit_15_dagger_240',\n",
              " 'crossvit_15_dagger_408',\n",
              " 'crossvit_18_240',\n",
              " 'crossvit_18_dagger_240',\n",
              " 'crossvit_18_dagger_408',\n",
              " 'crossvit_base_240',\n",
              " 'crossvit_small_240',\n",
              " 'crossvit_tiny_240',\n",
              " 'cs3darknet_focus_l',\n",
              " 'cs3darknet_focus_m',\n",
              " 'cs3darknet_focus_s',\n",
              " 'cs3darknet_focus_x',\n",
              " 'cs3darknet_l',\n",
              " 'cs3darknet_m',\n",
              " 'cs3darknet_s',\n",
              " 'cs3darknet_x',\n",
              " 'cs3edgenet_x',\n",
              " 'cs3se_edgenet_x',\n",
              " 'cs3sedarknet_l',\n",
              " 'cs3sedarknet_x',\n",
              " 'cs3sedarknet_xdw',\n",
              " 'cspdarknet53',\n",
              " 'cspresnet50',\n",
              " 'cspresnet50d',\n",
              " 'cspresnet50w',\n",
              " 'cspresnext50',\n",
              " 'darknet17',\n",
              " 'darknet21',\n",
              " 'darknet53',\n",
              " 'darknetaa53',\n",
              " 'deit3_base_patch16_224',\n",
              " 'deit3_base_patch16_224_in21ft1k',\n",
              " 'deit3_base_patch16_384',\n",
              " 'deit3_base_patch16_384_in21ft1k',\n",
              " 'deit3_huge_patch14_224',\n",
              " 'deit3_huge_patch14_224_in21ft1k',\n",
              " 'deit3_large_patch16_224',\n",
              " 'deit3_large_patch16_224_in21ft1k',\n",
              " 'deit3_large_patch16_384',\n",
              " 'deit3_large_patch16_384_in21ft1k',\n",
              " 'deit3_medium_patch16_224',\n",
              " 'deit3_medium_patch16_224_in21ft1k',\n",
              " 'deit3_small_patch16_224',\n",
              " 'deit3_small_patch16_224_in21ft1k',\n",
              " 'deit3_small_patch16_384',\n",
              " 'deit3_small_patch16_384_in21ft1k',\n",
              " 'deit_base_distilled_patch16_224',\n",
              " 'deit_base_distilled_patch16_384',\n",
              " 'deit_base_patch16_224',\n",
              " 'deit_base_patch16_384',\n",
              " 'deit_small_distilled_patch16_224',\n",
              " 'deit_small_patch16_224',\n",
              " 'deit_tiny_distilled_patch16_224',\n",
              " 'deit_tiny_patch16_224',\n",
              " 'densenet121',\n",
              " 'densenet121d',\n",
              " 'densenet161',\n",
              " 'densenet169',\n",
              " 'densenet201',\n",
              " 'densenet264',\n",
              " 'densenet264d_iabn',\n",
              " 'densenetblur121d',\n",
              " 'dla34',\n",
              " 'dla46_c',\n",
              " 'dla46x_c',\n",
              " 'dla60',\n",
              " 'dla60_res2net',\n",
              " 'dla60_res2next',\n",
              " 'dla60x',\n",
              " 'dla60x_c',\n",
              " 'dla102',\n",
              " 'dla102x',\n",
              " 'dla102x2',\n",
              " 'dla169',\n",
              " 'dm_nfnet_f0',\n",
              " 'dm_nfnet_f1',\n",
              " 'dm_nfnet_f2',\n",
              " 'dm_nfnet_f3',\n",
              " 'dm_nfnet_f4',\n",
              " 'dm_nfnet_f5',\n",
              " 'dm_nfnet_f6',\n",
              " 'dpn68',\n",
              " 'dpn68b',\n",
              " 'dpn92',\n",
              " 'dpn98',\n",
              " 'dpn107',\n",
              " 'dpn131',\n",
              " 'eca_botnext26ts_256',\n",
              " 'eca_halonext26ts',\n",
              " 'eca_nfnet_l0',\n",
              " 'eca_nfnet_l1',\n",
              " 'eca_nfnet_l2',\n",
              " 'eca_nfnet_l3',\n",
              " 'eca_resnet33ts',\n",
              " 'eca_resnext26ts',\n",
              " 'eca_vovnet39b',\n",
              " 'ecaresnet26t',\n",
              " 'ecaresnet50d',\n",
              " 'ecaresnet50d_pruned',\n",
              " 'ecaresnet50t',\n",
              " 'ecaresnet101d',\n",
              " 'ecaresnet101d_pruned',\n",
              " 'ecaresnet200d',\n",
              " 'ecaresnet269d',\n",
              " 'ecaresnetlight',\n",
              " 'ecaresnext26t_32x4d',\n",
              " 'ecaresnext50t_32x4d',\n",
              " 'edgenext_base',\n",
              " 'edgenext_small',\n",
              " 'edgenext_small_rw',\n",
              " 'edgenext_x_small',\n",
              " 'edgenext_xx_small',\n",
              " 'efficientformer_l1',\n",
              " 'efficientformer_l3',\n",
              " 'efficientformer_l7',\n",
              " 'efficientnet_b0',\n",
              " 'efficientnet_b0_g8_gn',\n",
              " 'efficientnet_b0_g16_evos',\n",
              " 'efficientnet_b0_gn',\n",
              " 'efficientnet_b1',\n",
              " 'efficientnet_b1_pruned',\n",
              " 'efficientnet_b2',\n",
              " 'efficientnet_b2_pruned',\n",
              " 'efficientnet_b2a',\n",
              " 'efficientnet_b3',\n",
              " 'efficientnet_b3_g8_gn',\n",
              " 'efficientnet_b3_gn',\n",
              " 'efficientnet_b3_pruned',\n",
              " 'efficientnet_b3a',\n",
              " 'efficientnet_b4',\n",
              " 'efficientnet_b5',\n",
              " 'efficientnet_b6',\n",
              " 'efficientnet_b7',\n",
              " 'efficientnet_b8',\n",
              " 'efficientnet_cc_b0_4e',\n",
              " 'efficientnet_cc_b0_8e',\n",
              " 'efficientnet_cc_b1_8e',\n",
              " 'efficientnet_el',\n",
              " 'efficientnet_el_pruned',\n",
              " 'efficientnet_em',\n",
              " 'efficientnet_es',\n",
              " 'efficientnet_es_pruned',\n",
              " 'efficientnet_l2',\n",
              " 'efficientnet_lite0',\n",
              " 'efficientnet_lite1',\n",
              " 'efficientnet_lite2',\n",
              " 'efficientnet_lite3',\n",
              " 'efficientnet_lite4',\n",
              " 'efficientnetv2_l',\n",
              " 'efficientnetv2_m',\n",
              " 'efficientnetv2_rw_m',\n",
              " 'efficientnetv2_rw_s',\n",
              " 'efficientnetv2_rw_t',\n",
              " 'efficientnetv2_s',\n",
              " 'efficientnetv2_xl',\n",
              " 'ens_adv_inception_resnet_v2',\n",
              " 'ese_vovnet19b_dw',\n",
              " 'ese_vovnet19b_slim',\n",
              " 'ese_vovnet19b_slim_dw',\n",
              " 'ese_vovnet39b',\n",
              " 'ese_vovnet39b_evos',\n",
              " 'ese_vovnet57b',\n",
              " 'ese_vovnet99b',\n",
              " 'ese_vovnet99b_iabn',\n",
              " 'fbnetc_100',\n",
              " 'fbnetv3_b',\n",
              " 'fbnetv3_d',\n",
              " 'fbnetv3_g',\n",
              " 'gc_efficientnetv2_rw_t',\n",
              " 'gcresnet33ts',\n",
              " 'gcresnet50t',\n",
              " 'gcresnext26ts',\n",
              " 'gcresnext50ts',\n",
              " 'gcvit_base',\n",
              " 'gcvit_small',\n",
              " 'gcvit_tiny',\n",
              " 'gcvit_xtiny',\n",
              " 'gcvit_xxtiny',\n",
              " 'gernet_l',\n",
              " 'gernet_m',\n",
              " 'gernet_s',\n",
              " 'ghostnet_050',\n",
              " 'ghostnet_100',\n",
              " 'ghostnet_130',\n",
              " 'gluon_inception_v3',\n",
              " 'gluon_resnet18_v1b',\n",
              " 'gluon_resnet34_v1b',\n",
              " 'gluon_resnet50_v1b',\n",
              " 'gluon_resnet50_v1c',\n",
              " 'gluon_resnet50_v1d',\n",
              " 'gluon_resnet50_v1s',\n",
              " 'gluon_resnet101_v1b',\n",
              " 'gluon_resnet101_v1c',\n",
              " 'gluon_resnet101_v1d',\n",
              " 'gluon_resnet101_v1s',\n",
              " 'gluon_resnet152_v1b',\n",
              " 'gluon_resnet152_v1c',\n",
              " 'gluon_resnet152_v1d',\n",
              " 'gluon_resnet152_v1s',\n",
              " 'gluon_resnext50_32x4d',\n",
              " 'gluon_resnext101_32x4d',\n",
              " 'gluon_resnext101_64x4d',\n",
              " 'gluon_senet154',\n",
              " 'gluon_seresnext50_32x4d',\n",
              " 'gluon_seresnext101_32x4d',\n",
              " 'gluon_seresnext101_64x4d',\n",
              " 'gluon_xception65',\n",
              " 'gmixer_12_224',\n",
              " 'gmixer_24_224',\n",
              " 'gmlp_b16_224',\n",
              " 'gmlp_s16_224',\n",
              " 'gmlp_ti16_224',\n",
              " 'halo2botnet50ts_256',\n",
              " 'halonet26t',\n",
              " 'halonet50ts',\n",
              " 'halonet_h1',\n",
              " 'haloregnetz_b',\n",
              " 'hardcorenas_a',\n",
              " 'hardcorenas_b',\n",
              " 'hardcorenas_c',\n",
              " 'hardcorenas_d',\n",
              " 'hardcorenas_e',\n",
              " 'hardcorenas_f',\n",
              " 'hrnet_w18',\n",
              " 'hrnet_w18_small',\n",
              " 'hrnet_w18_small_v2',\n",
              " 'hrnet_w30',\n",
              " 'hrnet_w32',\n",
              " 'hrnet_w40',\n",
              " 'hrnet_w44',\n",
              " 'hrnet_w48',\n",
              " 'hrnet_w64',\n",
              " 'ig_resnext101_32x8d',\n",
              " 'ig_resnext101_32x16d',\n",
              " 'ig_resnext101_32x32d',\n",
              " 'ig_resnext101_32x48d',\n",
              " 'inception_resnet_v2',\n",
              " 'inception_v3',\n",
              " 'inception_v4',\n",
              " 'jx_nest_base',\n",
              " 'jx_nest_small',\n",
              " 'jx_nest_tiny',\n",
              " 'lambda_resnet26rpt_256',\n",
              " 'lambda_resnet26t',\n",
              " 'lambda_resnet50ts',\n",
              " 'lamhalobotnet50ts_256',\n",
              " 'lcnet_035',\n",
              " 'lcnet_050',\n",
              " 'lcnet_075',\n",
              " 'lcnet_100',\n",
              " 'lcnet_150',\n",
              " 'legacy_senet154',\n",
              " 'legacy_seresnet18',\n",
              " 'legacy_seresnet34',\n",
              " 'legacy_seresnet50',\n",
              " 'legacy_seresnet101',\n",
              " 'legacy_seresnet152',\n",
              " 'legacy_seresnext26_32x4d',\n",
              " 'legacy_seresnext50_32x4d',\n",
              " 'legacy_seresnext101_32x4d',\n",
              " 'levit_128',\n",
              " 'levit_128s',\n",
              " 'levit_192',\n",
              " 'levit_256',\n",
              " 'levit_256d',\n",
              " 'levit_384',\n",
              " 'maxvit_base_224',\n",
              " 'maxvit_large_224',\n",
              " 'maxvit_nano_rw_256',\n",
              " 'maxvit_pico_rw_256',\n",
              " 'maxvit_rmlp_nano_rw_256',\n",
              " 'maxvit_rmlp_pico_rw_256',\n",
              " 'maxvit_rmlp_small_rw_224',\n",
              " 'maxvit_rmlp_small_rw_256',\n",
              " 'maxvit_rmlp_tiny_rw_256',\n",
              " 'maxvit_small_224',\n",
              " 'maxvit_tiny_224',\n",
              " 'maxvit_tiny_pm_256',\n",
              " 'maxvit_tiny_rw_224',\n",
              " 'maxvit_tiny_rw_256',\n",
              " 'maxvit_xlarge_224',\n",
              " 'maxxvit_nano_rw_256',\n",
              " 'maxxvit_small_rw_256',\n",
              " 'maxxvit_tiny_rw_256',\n",
              " 'mixer_b16_224',\n",
              " 'mixer_b16_224_in21k',\n",
              " 'mixer_b16_224_miil',\n",
              " 'mixer_b16_224_miil_in21k',\n",
              " 'mixer_b32_224',\n",
              " 'mixer_l16_224',\n",
              " 'mixer_l16_224_in21k',\n",
              " 'mixer_l32_224',\n",
              " 'mixer_s16_224',\n",
              " 'mixer_s32_224',\n",
              " 'mixnet_l',\n",
              " 'mixnet_m',\n",
              " 'mixnet_s',\n",
              " 'mixnet_xl',\n",
              " 'mixnet_xxl',\n",
              " 'mnasnet_050',\n",
              " 'mnasnet_075',\n",
              " 'mnasnet_100',\n",
              " 'mnasnet_140',\n",
              " 'mnasnet_a1',\n",
              " 'mnasnet_b1',\n",
              " 'mnasnet_small',\n",
              " 'mobilenetv2_035',\n",
              " 'mobilenetv2_050',\n",
              " 'mobilenetv2_075',\n",
              " 'mobilenetv2_100',\n",
              " 'mobilenetv2_110d',\n",
              " 'mobilenetv2_120d',\n",
              " 'mobilenetv2_140',\n",
              " 'mobilenetv3_large_075',\n",
              " 'mobilenetv3_large_100',\n",
              " 'mobilenetv3_large_100_miil',\n",
              " 'mobilenetv3_large_100_miil_in21k',\n",
              " 'mobilenetv3_rw',\n",
              " 'mobilenetv3_small_050',\n",
              " 'mobilenetv3_small_075',\n",
              " 'mobilenetv3_small_100',\n",
              " 'mobilevit_s',\n",
              " 'mobilevit_xs',\n",
              " 'mobilevit_xxs',\n",
              " 'mobilevitv2_050',\n",
              " 'mobilevitv2_075',\n",
              " 'mobilevitv2_100',\n",
              " 'mobilevitv2_125',\n",
              " 'mobilevitv2_150',\n",
              " 'mobilevitv2_150_384_in22ft1k',\n",
              " 'mobilevitv2_150_in22ft1k',\n",
              " 'mobilevitv2_175',\n",
              " 'mobilevitv2_175_384_in22ft1k',\n",
              " 'mobilevitv2_175_in22ft1k',\n",
              " 'mobilevitv2_200',\n",
              " 'mobilevitv2_200_384_in22ft1k',\n",
              " 'mobilevitv2_200_in22ft1k',\n",
              " 'mvitv2_base',\n",
              " 'mvitv2_large',\n",
              " 'mvitv2_small',\n",
              " 'mvitv2_small_cls',\n",
              " 'mvitv2_tiny',\n",
              " 'nasnetalarge',\n",
              " 'nest_base',\n",
              " 'nest_small',\n",
              " 'nest_tiny',\n",
              " 'nf_ecaresnet26',\n",
              " 'nf_ecaresnet50',\n",
              " 'nf_ecaresnet101',\n",
              " 'nf_regnet_b0',\n",
              " 'nf_regnet_b1',\n",
              " 'nf_regnet_b2',\n",
              " 'nf_regnet_b3',\n",
              " 'nf_regnet_b4',\n",
              " 'nf_regnet_b5',\n",
              " 'nf_resnet26',\n",
              " 'nf_resnet50',\n",
              " 'nf_resnet101',\n",
              " 'nf_seresnet26',\n",
              " 'nf_seresnet50',\n",
              " 'nf_seresnet101',\n",
              " 'nfnet_f0',\n",
              " 'nfnet_f1',\n",
              " 'nfnet_f2',\n",
              " 'nfnet_f3',\n",
              " 'nfnet_f4',\n",
              " 'nfnet_f5',\n",
              " 'nfnet_f6',\n",
              " 'nfnet_f7',\n",
              " 'nfnet_l0',\n",
              " 'pit_b_224',\n",
              " 'pit_b_distilled_224',\n",
              " 'pit_s_224',\n",
              " 'pit_s_distilled_224',\n",
              " 'pit_ti_224',\n",
              " 'pit_ti_distilled_224',\n",
              " 'pit_xs_224',\n",
              " 'pit_xs_distilled_224',\n",
              " 'pnasnet5large',\n",
              " 'poolformer_m36',\n",
              " 'poolformer_m48',\n",
              " 'poolformer_s12',\n",
              " 'poolformer_s24',\n",
              " 'poolformer_s36',\n",
              " 'pvt_v2_b0',\n",
              " 'pvt_v2_b1',\n",
              " 'pvt_v2_b2',\n",
              " 'pvt_v2_b2_li',\n",
              " 'pvt_v2_b3',\n",
              " 'pvt_v2_b4',\n",
              " 'pvt_v2_b5',\n",
              " 'regnetv_040',\n",
              " 'regnetv_064',\n",
              " 'regnetx_002',\n",
              " 'regnetx_004',\n",
              " 'regnetx_006',\n",
              " 'regnetx_008',\n",
              " 'regnetx_016',\n",
              " 'regnetx_032',\n",
              " 'regnetx_040',\n",
              " 'regnetx_064',\n",
              " 'regnetx_080',\n",
              " 'regnetx_120',\n",
              " 'regnetx_160',\n",
              " 'regnetx_320',\n",
              " 'regnety_002',\n",
              " 'regnety_004',\n",
              " 'regnety_006',\n",
              " 'regnety_008',\n",
              " 'regnety_016',\n",
              " 'regnety_032',\n",
              " 'regnety_040',\n",
              " 'regnety_040s_gn',\n",
              " 'regnety_064',\n",
              " 'regnety_080',\n",
              " 'regnety_120',\n",
              " 'regnety_160',\n",
              " 'regnety_320',\n",
              " 'regnetz_005',\n",
              " 'regnetz_040',\n",
              " 'regnetz_040h',\n",
              " 'regnetz_b16',\n",
              " 'regnetz_b16_evos',\n",
              " 'regnetz_c16',\n",
              " 'regnetz_c16_evos',\n",
              " 'regnetz_d8',\n",
              " 'regnetz_d8_evos',\n",
              " 'regnetz_d32',\n",
              " 'regnetz_e8',\n",
              " 'repvgg_a2',\n",
              " 'repvgg_b0',\n",
              " 'repvgg_b1',\n",
              " 'repvgg_b1g4',\n",
              " 'repvgg_b2',\n",
              " 'repvgg_b2g4',\n",
              " 'repvgg_b3',\n",
              " 'repvgg_b3g4',\n",
              " 'res2net50_14w_8s',\n",
              " 'res2net50_26w_4s',\n",
              " 'res2net50_26w_6s',\n",
              " 'res2net50_26w_8s',\n",
              " 'res2net50_48w_2s',\n",
              " 'res2net101_26w_4s',\n",
              " 'res2next50',\n",
              " 'resmlp_12_224',\n",
              " 'resmlp_12_224_dino',\n",
              " 'resmlp_12_distilled_224',\n",
              " 'resmlp_24_224',\n",
              " 'resmlp_24_224_dino',\n",
              " 'resmlp_24_distilled_224',\n",
              " 'resmlp_36_224',\n",
              " 'resmlp_36_distilled_224',\n",
              " 'resmlp_big_24_224',\n",
              " 'resmlp_big_24_224_in22ft1k',\n",
              " 'resmlp_big_24_distilled_224',\n",
              " 'resnest14d',\n",
              " 'resnest26d',\n",
              " 'resnest50d',\n",
              " 'resnest50d_1s4x24d',\n",
              " 'resnest50d_4s2x40d',\n",
              " 'resnest101e',\n",
              " 'resnest200e',\n",
              " 'resnest269e',\n",
              " 'resnet10t',\n",
              " 'resnet14t',\n",
              " 'resnet18',\n",
              " 'resnet18d',\n",
              " 'resnet26',\n",
              " 'resnet26d',\n",
              " 'resnet26t',\n",
              " 'resnet32ts',\n",
              " 'resnet33ts',\n",
              " 'resnet34',\n",
              " 'resnet34d',\n",
              " 'resnet50',\n",
              " 'resnet50_gn',\n",
              " 'resnet50d',\n",
              " 'resnet50t',\n",
              " 'resnet51q',\n",
              " 'resnet61q',\n",
              " 'resnet101',\n",
              " 'resnet101d',\n",
              " 'resnet152',\n",
              " 'resnet152d',\n",
              " 'resnet200',\n",
              " 'resnet200d',\n",
              " 'resnetaa50',\n",
              " 'resnetaa50d',\n",
              " 'resnetaa101d',\n",
              " 'resnetblur18',\n",
              " 'resnetblur50',\n",
              " 'resnetblur50d',\n",
              " 'resnetblur101d',\n",
              " 'resnetrs50',\n",
              " 'resnetrs101',\n",
              " 'resnetrs152',\n",
              " 'resnetrs200',\n",
              " 'resnetrs270',\n",
              " 'resnetrs350',\n",
              " 'resnetrs420',\n",
              " 'resnetv2_50',\n",
              " 'resnetv2_50d',\n",
              " 'resnetv2_50d_evob',\n",
              " 'resnetv2_50d_evos',\n",
              " 'resnetv2_50d_frn',\n",
              " 'resnetv2_50d_gn',\n",
              " 'resnetv2_50t',\n",
              " 'resnetv2_50x1_bit_distilled',\n",
              " 'resnetv2_50x1_bitm',\n",
              " 'resnetv2_50x1_bitm_in21k',\n",
              " 'resnetv2_50x3_bitm',\n",
              " 'resnetv2_50x3_bitm_in21k',\n",
              " 'resnetv2_101',\n",
              " 'resnetv2_101d',\n",
              " 'resnetv2_101x1_bitm',\n",
              " 'resnetv2_101x1_bitm_in21k',\n",
              " 'resnetv2_101x3_bitm',\n",
              " 'resnetv2_101x3_bitm_in21k',\n",
              " 'resnetv2_152',\n",
              " 'resnetv2_152d',\n",
              " 'resnetv2_152x2_bit_teacher',\n",
              " 'resnetv2_152x2_bit_teacher_384',\n",
              " 'resnetv2_152x2_bitm',\n",
              " 'resnetv2_152x2_bitm_in21k',\n",
              " 'resnetv2_152x4_bitm',\n",
              " 'resnetv2_152x4_bitm_in21k',\n",
              " 'resnext26ts',\n",
              " 'resnext50_32x4d',\n",
              " 'resnext50d_32x4d',\n",
              " 'resnext101_32x4d',\n",
              " 'resnext101_32x8d',\n",
              " 'resnext101_64x4d',\n",
              " 'rexnet_100',\n",
              " 'rexnet_130',\n",
              " 'rexnet_150',\n",
              " 'rexnet_200',\n",
              " 'rexnetr_100',\n",
              " 'rexnetr_130',\n",
              " 'rexnetr_150',\n",
              " 'rexnetr_200',\n",
              " 'sebotnet33ts_256',\n",
              " 'sedarknet21',\n",
              " 'sehalonet33ts',\n",
              " 'selecsls42',\n",
              " 'selecsls42b',\n",
              " 'selecsls60',\n",
              " 'selecsls60b',\n",
              " 'selecsls84',\n",
              " 'semnasnet_050',\n",
              " 'semnasnet_075',\n",
              " 'semnasnet_100',\n",
              " 'semnasnet_140',\n",
              " 'semobilevit_s',\n",
              " 'senet154',\n",
              " 'sequencer2d_l',\n",
              " 'sequencer2d_m',\n",
              " 'sequencer2d_s',\n",
              " 'seresnet18',\n",
              " 'seresnet33ts',\n",
              " 'seresnet34',\n",
              " 'seresnet50',\n",
              " 'seresnet50t',\n",
              " 'seresnet101',\n",
              " 'seresnet152',\n",
              " 'seresnet152d',\n",
              " 'seresnet200d',\n",
              " 'seresnet269d',\n",
              " 'seresnetaa50d',\n",
              " 'seresnext26d_32x4d',\n",
              " 'seresnext26t_32x4d',\n",
              " 'seresnext26tn_32x4d',\n",
              " 'seresnext26ts',\n",
              " 'seresnext50_32x4d',\n",
              " 'seresnext101_32x4d',\n",
              " 'seresnext101_32x8d',\n",
              " 'seresnext101d_32x8d',\n",
              " 'seresnextaa101d_32x8d',\n",
              " 'skresnet18',\n",
              " 'skresnet34',\n",
              " 'skresnet50',\n",
              " 'skresnet50d',\n",
              " 'skresnext50_32x4d',\n",
              " 'spnasnet_100',\n",
              " 'ssl_resnet18',\n",
              " 'ssl_resnet50',\n",
              " 'ssl_resnext50_32x4d',\n",
              " 'ssl_resnext101_32x4d',\n",
              " 'ssl_resnext101_32x8d',\n",
              " 'ssl_resnext101_32x16d',\n",
              " 'swin_base_patch4_window7_224',\n",
              " 'swin_base_patch4_window7_224_in22k',\n",
              " 'swin_base_patch4_window12_384',\n",
              " 'swin_base_patch4_window12_384_in22k',\n",
              " 'swin_large_patch4_window7_224',\n",
              " 'swin_large_patch4_window7_224_in22k',\n",
              " 'swin_large_patch4_window12_384',\n",
              " 'swin_large_patch4_window12_384_in22k',\n",
              " 'swin_s3_base_224',\n",
              " 'swin_s3_small_224',\n",
              " 'swin_s3_tiny_224',\n",
              " 'swin_small_patch4_window7_224',\n",
              " 'swin_tiny_patch4_window7_224',\n",
              " 'swinv2_base_window8_256',\n",
              " 'swinv2_base_window12_192_22k',\n",
              " 'swinv2_base_window12to16_192to256_22kft1k',\n",
              " 'swinv2_base_window12to24_192to384_22kft1k',\n",
              " 'swinv2_base_window16_256',\n",
              " 'swinv2_cr_base_224',\n",
              " 'swinv2_cr_base_384',\n",
              " 'swinv2_cr_base_ns_224',\n",
              " 'swinv2_cr_giant_224',\n",
              " 'swinv2_cr_giant_384',\n",
              " 'swinv2_cr_huge_224',\n",
              " 'swinv2_cr_huge_384',\n",
              " 'swinv2_cr_large_224',\n",
              " 'swinv2_cr_large_384',\n",
              " 'swinv2_cr_small_224',\n",
              " 'swinv2_cr_small_384',\n",
              " 'swinv2_cr_small_ns_224',\n",
              " 'swinv2_cr_tiny_224',\n",
              " 'swinv2_cr_tiny_384',\n",
              " 'swinv2_cr_tiny_ns_224',\n",
              " 'swinv2_large_window12_192_22k',\n",
              " 'swinv2_large_window12to16_192to256_22kft1k',\n",
              " 'swinv2_large_window12to24_192to384_22kft1k',\n",
              " 'swinv2_small_window8_256',\n",
              " 'swinv2_small_window16_256',\n",
              " 'swinv2_tiny_window8_256',\n",
              " 'swinv2_tiny_window16_256',\n",
              " 'swsl_resnet18',\n",
              " 'swsl_resnet50',\n",
              " 'swsl_resnext50_32x4d',\n",
              " 'swsl_resnext101_32x4d',\n",
              " 'swsl_resnext101_32x8d',\n",
              " 'swsl_resnext101_32x16d',\n",
              " 'tf_efficientnet_b0',\n",
              " 'tf_efficientnet_b0_ap',\n",
              " 'tf_efficientnet_b0_ns',\n",
              " 'tf_efficientnet_b1',\n",
              " 'tf_efficientnet_b1_ap',\n",
              " 'tf_efficientnet_b1_ns',\n",
              " 'tf_efficientnet_b2',\n",
              " 'tf_efficientnet_b2_ap',\n",
              " 'tf_efficientnet_b2_ns',\n",
              " 'tf_efficientnet_b3',\n",
              " 'tf_efficientnet_b3_ap',\n",
              " 'tf_efficientnet_b3_ns',\n",
              " 'tf_efficientnet_b4',\n",
              " 'tf_efficientnet_b4_ap',\n",
              " 'tf_efficientnet_b4_ns',\n",
              " 'tf_efficientnet_b5',\n",
              " 'tf_efficientnet_b5_ap',\n",
              " 'tf_efficientnet_b5_ns',\n",
              " 'tf_efficientnet_b6',\n",
              " 'tf_efficientnet_b6_ap',\n",
              " 'tf_efficientnet_b6_ns',\n",
              " 'tf_efficientnet_b7',\n",
              " 'tf_efficientnet_b7_ap',\n",
              " 'tf_efficientnet_b7_ns',\n",
              " 'tf_efficientnet_b8',\n",
              " 'tf_efficientnet_b8_ap',\n",
              " 'tf_efficientnet_cc_b0_4e',\n",
              " 'tf_efficientnet_cc_b0_8e',\n",
              " 'tf_efficientnet_cc_b1_8e',\n",
              " 'tf_efficientnet_el',\n",
              " 'tf_efficientnet_em',\n",
              " 'tf_efficientnet_es',\n",
              " 'tf_efficientnet_l2_ns',\n",
              " 'tf_efficientnet_l2_ns_475',\n",
              " 'tf_efficientnet_lite0',\n",
              " 'tf_efficientnet_lite1',\n",
              " 'tf_efficientnet_lite2',\n",
              " 'tf_efficientnet_lite3',\n",
              " 'tf_efficientnet_lite4',\n",
              " 'tf_efficientnetv2_b0',\n",
              " 'tf_efficientnetv2_b1',\n",
              " 'tf_efficientnetv2_b2',\n",
              " 'tf_efficientnetv2_b3',\n",
              " 'tf_efficientnetv2_l',\n",
              " 'tf_efficientnetv2_l_in21ft1k',\n",
              " 'tf_efficientnetv2_l_in21k',\n",
              " 'tf_efficientnetv2_m',\n",
              " 'tf_efficientnetv2_m_in21ft1k',\n",
              " 'tf_efficientnetv2_m_in21k',\n",
              " 'tf_efficientnetv2_s',\n",
              " 'tf_efficientnetv2_s_in21ft1k',\n",
              " 'tf_efficientnetv2_s_in21k',\n",
              " 'tf_efficientnetv2_xl_in21ft1k',\n",
              " 'tf_efficientnetv2_xl_in21k',\n",
              " 'tf_inception_v3',\n",
              " 'tf_mixnet_l',\n",
              " 'tf_mixnet_m',\n",
              " 'tf_mixnet_s',\n",
              " 'tf_mobilenetv3_large_075',\n",
              " 'tf_mobilenetv3_large_100',\n",
              " 'tf_mobilenetv3_large_minimal_100',\n",
              " 'tf_mobilenetv3_small_075',\n",
              " 'tf_mobilenetv3_small_100',\n",
              " 'tf_mobilenetv3_small_minimal_100',\n",
              " 'tinynet_a',\n",
              " 'tinynet_b',\n",
              " 'tinynet_c',\n",
              " 'tinynet_d',\n",
              " 'tinynet_e',\n",
              " 'tnt_b_patch16_224',\n",
              " 'tnt_s_patch16_224',\n",
              " 'tresnet_l',\n",
              " 'tresnet_l_448',\n",
              " 'tresnet_m',\n",
              " 'tresnet_m_448',\n",
              " 'tresnet_m_miil_in21k',\n",
              " 'tresnet_v2_l',\n",
              " 'tresnet_xl',\n",
              " 'tresnet_xl_448',\n",
              " 'tv_densenet121',\n",
              " 'tv_resnet34',\n",
              " 'tv_resnet50',\n",
              " 'tv_resnet101',\n",
              " 'tv_resnet152',\n",
              " 'tv_resnext50_32x4d',\n",
              " 'twins_pcpvt_base',\n",
              " 'twins_pcpvt_large',\n",
              " 'twins_pcpvt_small',\n",
              " 'twins_svt_base',\n",
              " 'twins_svt_large',\n",
              " 'twins_svt_small',\n",
              " 'vgg11',\n",
              " 'vgg11_bn',\n",
              " 'vgg13',\n",
              " 'vgg13_bn',\n",
              " 'vgg16',\n",
              " 'vgg16_bn',\n",
              " 'vgg19',\n",
              " 'vgg19_bn',\n",
              " 'visformer_small',\n",
              " 'visformer_tiny',\n",
              " 'vit_base_patch8_224',\n",
              " 'vit_base_patch8_224_dino',\n",
              " 'vit_base_patch8_224_in21k',\n",
              " 'vit_base_patch16_18x2_224',\n",
              " 'vit_base_patch16_224',\n",
              " 'vit_base_patch16_224_dino',\n",
              " 'vit_base_patch16_224_in21k',\n",
              " 'vit_base_patch16_224_miil',\n",
              " 'vit_base_patch16_224_miil_in21k',\n",
              " 'vit_base_patch16_224_sam',\n",
              " 'vit_base_patch16_384',\n",
              " 'vit_base_patch16_plus_240',\n",
              " 'vit_base_patch16_rpn_224',\n",
              " 'vit_base_patch32_224',\n",
              " 'vit_base_patch32_224_clip_laion2b',\n",
              " 'vit_base_patch32_224_in21k',\n",
              " 'vit_base_patch32_224_sam',\n",
              " 'vit_base_patch32_384',\n",
              " 'vit_base_patch32_plus_256',\n",
              " 'vit_base_r26_s32_224',\n",
              " 'vit_base_r50_s16_224',\n",
              " 'vit_base_r50_s16_224_in21k',\n",
              " 'vit_base_r50_s16_384',\n",
              " 'vit_base_resnet26d_224',\n",
              " 'vit_base_resnet50_224_in21k',\n",
              " 'vit_base_resnet50_384',\n",
              " 'vit_base_resnet50d_224',\n",
              " 'vit_giant_patch14_224',\n",
              " 'vit_giant_patch14_224_clip_laion2b',\n",
              " 'vit_gigantic_patch14_224',\n",
              " 'vit_huge_patch14_224',\n",
              " 'vit_huge_patch14_224_clip_laion2b',\n",
              " 'vit_huge_patch14_224_in21k',\n",
              " 'vit_large_patch14_224',\n",
              " 'vit_large_patch14_224_clip_laion2b',\n",
              " 'vit_large_patch16_224',\n",
              " 'vit_large_patch16_224_in21k',\n",
              " 'vit_large_patch16_384',\n",
              " 'vit_large_patch32_224',\n",
              " 'vit_large_patch32_224_in21k',\n",
              " 'vit_large_patch32_384',\n",
              " 'vit_large_r50_s32_224',\n",
              " 'vit_large_r50_s32_224_in21k',\n",
              " 'vit_large_r50_s32_384',\n",
              " 'vit_relpos_base_patch16_224',\n",
              " 'vit_relpos_base_patch16_cls_224',\n",
              " 'vit_relpos_base_patch16_clsgap_224',\n",
              " 'vit_relpos_base_patch16_plus_240',\n",
              " 'vit_relpos_base_patch16_rpn_224',\n",
              " 'vit_relpos_base_patch32_plus_rpn_256',\n",
              " 'vit_relpos_medium_patch16_224',\n",
              " 'vit_relpos_medium_patch16_cls_224',\n",
              " 'vit_relpos_medium_patch16_rpn_224',\n",
              " 'vit_relpos_small_patch16_224',\n",
              " 'vit_relpos_small_patch16_rpn_224',\n",
              " 'vit_small_patch8_224_dino',\n",
              " 'vit_small_patch16_18x2_224',\n",
              " 'vit_small_patch16_36x1_224',\n",
              " 'vit_small_patch16_224',\n",
              " 'vit_small_patch16_224_dino',\n",
              " 'vit_small_patch16_224_in21k',\n",
              " 'vit_small_patch16_384',\n",
              " 'vit_small_patch32_224',\n",
              " 'vit_small_patch32_224_in21k',\n",
              " 'vit_small_patch32_384',\n",
              " 'vit_small_r26_s32_224',\n",
              " 'vit_small_r26_s32_224_in21k',\n",
              " 'vit_small_r26_s32_384',\n",
              " 'vit_small_resnet26d_224',\n",
              " 'vit_small_resnet50d_s16_224',\n",
              " 'vit_srelpos_medium_patch16_224',\n",
              " 'vit_srelpos_small_patch16_224',\n",
              " 'vit_tiny_patch16_224',\n",
              " 'vit_tiny_patch16_224_in21k',\n",
              " 'vit_tiny_patch16_384',\n",
              " 'vit_tiny_r_s16_p8_224',\n",
              " 'vit_tiny_r_s16_p8_224_in21k',\n",
              " 'vit_tiny_r_s16_p8_384',\n",
              " 'volo_d1_224',\n",
              " 'volo_d1_384',\n",
              " 'volo_d2_224',\n",
              " 'volo_d2_384',\n",
              " 'volo_d3_224',\n",
              " 'volo_d3_448',\n",
              " 'volo_d4_224',\n",
              " 'volo_d4_448',\n",
              " 'volo_d5_224',\n",
              " 'volo_d5_448',\n",
              " 'volo_d5_512',\n",
              " 'vovnet39a',\n",
              " 'vovnet57a',\n",
              " 'wide_resnet50_2',\n",
              " 'wide_resnet101_2',\n",
              " 'xception',\n",
              " 'xception41',\n",
              " 'xception41p',\n",
              " 'xception65',\n",
              " 'xception65p',\n",
              " 'xception71',\n",
              " 'xcit_large_24_p8_224',\n",
              " 'xcit_large_24_p8_224_dist',\n",
              " 'xcit_large_24_p8_384_dist',\n",
              " 'xcit_large_24_p16_224',\n",
              " 'xcit_large_24_p16_224_dist',\n",
              " 'xcit_large_24_p16_384_dist',\n",
              " 'xcit_medium_24_p8_224',\n",
              " 'xcit_medium_24_p8_224_dist',\n",
              " 'xcit_medium_24_p8_384_dist',\n",
              " 'xcit_medium_24_p16_224',\n",
              " 'xcit_medium_24_p16_224_dist',\n",
              " 'xcit_medium_24_p16_384_dist',\n",
              " 'xcit_nano_12_p8_224',\n",
              " 'xcit_nano_12_p8_224_dist',\n",
              " 'xcit_nano_12_p8_384_dist',\n",
              " 'xcit_nano_12_p16_224',\n",
              " 'xcit_nano_12_p16_224_dist',\n",
              " 'xcit_nano_12_p16_384_dist',\n",
              " 'xcit_small_12_p8_224',\n",
              " 'xcit_small_12_p8_224_dist',\n",
              " 'xcit_small_12_p8_384_dist',\n",
              " 'xcit_small_12_p16_224',\n",
              " 'xcit_small_12_p16_224_dist',\n",
              " 'xcit_small_12_p16_384_dist',\n",
              " 'xcit_small_24_p8_224',\n",
              " 'xcit_small_24_p8_224_dist',\n",
              " 'xcit_small_24_p8_384_dist',\n",
              " 'xcit_small_24_p16_224',\n",
              " 'xcit_small_24_p16_224_dist',\n",
              " 'xcit_small_24_p16_384_dist',\n",
              " 'xcit_tiny_12_p8_224',\n",
              " 'xcit_tiny_12_p8_224_dist',\n",
              " 'xcit_tiny_12_p8_384_dist',\n",
              " 'xcit_tiny_12_p16_224',\n",
              " 'xcit_tiny_12_p16_224_dist',\n",
              " 'xcit_tiny_12_p16_384_dist',\n",
              " 'xcit_tiny_24_p8_224',\n",
              " 'xcit_tiny_24_p8_224_dist',\n",
              " 'xcit_tiny_24_p8_384_dist',\n",
              " 'xcit_tiny_24_p16_224',\n",
              " 'xcit_tiny_24_p16_224_dist',\n",
              " 'xcit_tiny_24_p16_384_dist']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install timm\n",
        "import timm\n",
        "\n",
        "\n",
        "timm.list_models()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-ahq4zjKiQP"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(            \n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        logits = self.classifier(x)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os2IBshCP2bO"
      },
      "outputs": [],
      "source": [
        "net = LeNet5(n_classes = 10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IufYT0P4Zx",
        "outputId": "1b1f4898-35c6-45ef-9238-bc9e383ebed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.8632899314165116\n",
            "Epoch: 1,Batch : 1500, Loss:0.8623618266582489\n",
            "validation loss 360.4013366699219\n",
            "10.65\n",
            "Epoch: 2,Batch : 750, Loss:0.8578474785089493\n",
            "Epoch: 2,Batch : 1500, Loss:0.8472509235143661\n",
            "validation loss 352.2631530761719\n",
            "8.1\n",
            "Epoch: 3,Batch : 750, Loss:0.8407577077150344\n",
            "Epoch: 3,Batch : 1500, Loss:0.8377511608600616\n",
            "validation loss 349.51251220703125\n",
            "7.45\n",
            "Epoch: 4,Batch : 750, Loss:0.8345701067447663\n",
            "Epoch: 4,Batch : 1500, Loss:0.8298817946910858\n",
            "validation loss 346.21734619140625\n",
            "6.9\n",
            "Epoch: 5,Batch : 750, Loss:0.8265466728210449\n",
            "Epoch: 5,Batch : 1500, Loss:0.8238540161848068\n",
            "validation loss 343.687744140625\n",
            "7.6\n",
            "Epoch: 6,Batch : 750, Loss:0.8210046716928482\n",
            "Epoch: 6,Batch : 1500, Loss:0.817640862941742\n",
            "validation loss 340.8033447265625\n",
            "7.6\n",
            "Epoch: 7,Batch : 750, Loss:0.8141855626702309\n",
            "Epoch: 7,Batch : 1500, Loss:0.8110812964439392\n",
            "validation loss 338.1292724609375\n",
            "8.4\n",
            "Epoch: 8,Batch : 750, Loss:0.8078433132171631\n",
            "Epoch: 8,Batch : 1500, Loss:0.8064443021416664\n",
            "validation loss 335.78436279296875\n",
            "8.05\n",
            "Epoch: 9,Batch : 750, Loss:0.8023209210634231\n",
            "Epoch: 9,Batch : 1500, Loss:0.8016944132447242\n",
            "validation loss 333.7991638183594\n",
            "8.35\n",
            "Epoch: 10,Batch : 750, Loss:0.7978857699632644\n",
            "Epoch: 10,Batch : 1500, Loss:0.7966212297081947\n",
            "validation loss 332.154541015625\n",
            "8.6\n",
            "Epoch: 11,Batch : 750, Loss:0.79314311170578\n",
            "Epoch: 11,Batch : 1500, Loss:0.7927119830846786\n",
            "validation loss 330.414306640625\n",
            "9.0\n",
            "Epoch: 12,Batch : 750, Loss:0.7909195202589036\n",
            "Epoch: 12,Batch : 1500, Loss:0.78749394261837\n",
            "validation loss 329.0254821777344\n",
            "8.95\n",
            "Epoch: 13,Batch : 750, Loss:0.7863792996406556\n",
            "Epoch: 13,Batch : 1500, Loss:0.784693707883358\n",
            "validation loss 327.9298400878906\n",
            "7.9\n",
            "Epoch: 14,Batch : 750, Loss:0.7830760769248009\n",
            "Epoch: 14,Batch : 1500, Loss:0.7827440425753593\n",
            "validation loss 327.09930419921875\n",
            "8.1\n",
            "Epoch: 15,Batch : 750, Loss:0.7805073236227036\n",
            "Epoch: 15,Batch : 1500, Loss:0.780630424618721\n",
            "validation loss 325.8455810546875\n",
            "8.2\n",
            "Epoch: 16,Batch : 750, Loss:0.7795385731458664\n",
            "Epoch: 16,Batch : 1500, Loss:0.7764094383716583\n",
            "validation loss 325.144775390625\n",
            "8.5\n",
            "Epoch: 17,Batch : 750, Loss:0.7759364005923272\n",
            "Epoch: 17,Batch : 1500, Loss:0.7756128970980645\n",
            "validation loss 324.333740234375\n",
            "8.4\n",
            "Epoch: 18,Batch : 750, Loss:0.774593051314354\n",
            "Epoch: 18,Batch : 1500, Loss:0.7731572348475456\n",
            "validation loss 323.44232177734375\n",
            "8.05\n",
            "Epoch: 19,Batch : 750, Loss:0.7718705145716667\n",
            "Epoch: 19,Batch : 1500, Loss:0.7707885050773621\n",
            "validation loss 323.16497802734375\n",
            "8.75\n",
            "Epoch: 20,Batch : 750, Loss:0.7706714406013488\n",
            "Epoch: 20,Batch : 1500, Loss:0.7687193366289139\n",
            "validation loss 321.8344421386719\n",
            "8.85\n",
            "Epoch: 21,Batch : 750, Loss:0.7681307490468026\n",
            "Epoch: 21,Batch : 1500, Loss:0.7669758771657944\n",
            "validation loss 320.9377746582031\n",
            "9.15\n",
            "Epoch: 22,Batch : 750, Loss:0.765741961479187\n",
            "Epoch: 22,Batch : 1500, Loss:0.7655622866153717\n",
            "validation loss 320.2481384277344\n",
            "8.9\n",
            "Epoch: 23,Batch : 750, Loss:0.7622960377335548\n",
            "Epoch: 23,Batch : 1500, Loss:0.7650808085799217\n",
            "validation loss 319.4777526855469\n",
            "8.65\n",
            "Epoch: 24,Batch : 750, Loss:0.7610660247802734\n",
            "Epoch: 24,Batch : 1500, Loss:0.7622481653690338\n",
            "validation loss 318.8292236328125\n",
            "9.85\n",
            "Epoch: 25,Batch : 750, Loss:0.7604949029088021\n",
            "Epoch: 25,Batch : 1500, Loss:0.7585451794266701\n",
            "validation loss 318.08770751953125\n",
            "9.55\n",
            "Epoch: 26,Batch : 750, Loss:0.7573602455258369\n",
            "Epoch: 26,Batch : 1500, Loss:0.759134249150753\n",
            "validation loss 317.2827453613281\n",
            "9.6\n",
            "Epoch: 27,Batch : 750, Loss:0.7574226257801056\n",
            "Epoch: 27,Batch : 1500, Loss:0.7549784632921219\n",
            "validation loss 316.5497131347656\n",
            "9.55\n",
            "Epoch: 28,Batch : 750, Loss:0.7553468088507652\n",
            "Epoch: 28,Batch : 1500, Loss:0.7547704290151596\n",
            "validation loss 315.7781982421875\n",
            "10.0\n",
            "Epoch: 29,Batch : 750, Loss:0.7524763045310974\n",
            "Epoch: 29,Batch : 1500, Loss:0.7532618433237076\n",
            "validation loss 315.1544494628906\n",
            "10.3\n",
            "Epoch: 30,Batch : 750, Loss:0.7520357053875923\n",
            "Epoch: 30,Batch : 1500, Loss:0.7500220910906792\n",
            "validation loss 314.47943115234375\n",
            "10.4\n",
            "Epoch: 31,Batch : 750, Loss:0.7498295480012893\n",
            "Epoch: 31,Batch : 1500, Loss:0.748920495212078\n",
            "validation loss 314.4453430175781\n",
            "9.6\n",
            "Epoch: 32,Batch : 750, Loss:0.748205525457859\n",
            "Epoch: 32,Batch : 1500, Loss:0.7474491328597069\n",
            "validation loss 313.722412109375\n",
            "11.4\n",
            "Epoch: 33,Batch : 750, Loss:0.7472695725560188\n",
            "Epoch: 33,Batch : 1500, Loss:0.7457627476453781\n",
            "validation loss 312.7247009277344\n",
            "11.5\n",
            "Epoch: 34,Batch : 750, Loss:0.7448417631983757\n",
            "Epoch: 34,Batch : 1500, Loss:0.7453167734742164\n",
            "validation loss 312.43658447265625\n",
            "11.05\n",
            "Epoch: 35,Batch : 750, Loss:0.7453313122987747\n",
            "Epoch: 35,Batch : 1500, Loss:0.7422957422733307\n",
            "validation loss 312.0585021972656\n",
            "10.9\n",
            "Epoch: 36,Batch : 750, Loss:0.7428633123040199\n",
            "Epoch: 36,Batch : 1500, Loss:0.7419426686763764\n",
            "validation loss 311.64300537109375\n",
            "12.25\n",
            "Epoch: 37,Batch : 750, Loss:0.7422125603556633\n",
            "Epoch: 37,Batch : 1500, Loss:0.7403621140122414\n",
            "validation loss 311.2648010253906\n",
            "12.25\n",
            "Epoch: 38,Batch : 750, Loss:0.7397629325985908\n",
            "Epoch: 38,Batch : 1500, Loss:0.739378727376461\n",
            "validation loss 310.7884826660156\n",
            "12.05\n",
            "Epoch: 39,Batch : 750, Loss:0.7389480377435684\n",
            "Epoch: 39,Batch : 1500, Loss:0.7369192264080048\n",
            "validation loss 310.4381103515625\n",
            "12.25\n",
            "Epoch: 40,Batch : 750, Loss:0.7365459758639336\n",
            "Epoch: 40,Batch : 1500, Loss:0.7374676788449287\n",
            "validation loss 309.9595642089844\n",
            "12.75\n",
            "Epoch: 41,Batch : 750, Loss:0.7360395148992539\n",
            "Epoch: 41,Batch : 1500, Loss:0.7359111661911011\n",
            "validation loss 309.7289123535156\n",
            "12.5\n",
            "Epoch: 42,Batch : 750, Loss:0.7362373012900353\n",
            "Epoch: 42,Batch : 1500, Loss:0.7342406953573227\n",
            "validation loss 309.1181335449219\n",
            "13.6\n",
            "Epoch: 43,Batch : 750, Loss:0.7337625371813774\n",
            "Epoch: 43,Batch : 1500, Loss:0.733538355588913\n",
            "validation loss 308.6822814941406\n",
            "13.0\n",
            "Epoch: 44,Batch : 750, Loss:0.731965472638607\n",
            "Epoch: 44,Batch : 1500, Loss:0.7323863036036491\n",
            "validation loss 308.27935791015625\n",
            "14.35\n",
            "Epoch: 45,Batch : 750, Loss:0.7323324071764946\n",
            "Epoch: 45,Batch : 1500, Loss:0.7312287625670433\n",
            "validation loss 308.02081298828125\n",
            "12.95\n",
            "Epoch: 46,Batch : 750, Loss:0.7318958442211151\n",
            "Epoch: 46,Batch : 1500, Loss:0.7304661319851875\n",
            "validation loss 307.8404846191406\n",
            "14.7\n",
            "Epoch: 47,Batch : 750, Loss:0.7297500215768814\n",
            "Epoch: 47,Batch : 1500, Loss:0.7288096414208413\n",
            "validation loss 307.1560974121094\n",
            "14.6\n",
            "Epoch: 48,Batch : 750, Loss:0.7290460915565491\n",
            "Epoch: 48,Batch : 1500, Loss:0.7288948084115983\n",
            "validation loss 306.781005859375\n",
            "14.3\n",
            "Epoch: 49,Batch : 750, Loss:0.7280209599733353\n",
            "Epoch: 49,Batch : 1500, Loss:0.7277091289162636\n",
            "validation loss 307.4364013671875\n",
            "14.8\n",
            "Epoch: 50,Batch : 750, Loss:0.7264491016864777\n",
            "Epoch: 50,Batch : 1500, Loss:0.7269564401507378\n",
            "validation loss 306.2956237792969\n",
            "16.05\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_loss = 0.0\n",
        "    for k, data1 in enumerate(vaild_loader, 0):\n",
        "      val_inputs, val_label = data1[0].to(device), data1[1].to(device)\n",
        "      val_output = net(val_inputs)\n",
        "      v_loss = criterion(val_output, val_label)\n",
        "      val_loss += v_loss\n",
        "  print(\"validation loss {}\".format(val_loss))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data[0].to(device), data[1].to(device)\n",
        "      outputs= net(images)\n",
        "      _, predicted = torch.max(outputs.data,1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(\"testset Accuracy  : {}\".format(100* correct/total))\n",
        "  total=0\n",
        "  correct=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wd3JH4IS1aE",
        "outputId": "8fb88a40-26f9-4e0d-cb72-72097b8003c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 51,Batch : 750, Loss:0.7259917783141137\n",
            "Epoch: 51,Batch : 1500, Loss:0.7258416210412979\n",
            "validation loss 307.23040771484375\n",
            "testset Accuracy  : 16.65\n",
            "Epoch: 52,Batch : 750, Loss:0.7250983639359474\n",
            "Epoch: 52,Batch : 1500, Loss:0.7256888750791549\n",
            "validation loss 305.6312255859375\n",
            "testset Accuracy  : 15.2\n",
            "Epoch: 53,Batch : 750, Loss:0.7240996459722518\n",
            "Epoch: 53,Batch : 1500, Loss:0.7243660908341408\n",
            "validation loss 305.6039123535156\n",
            "testset Accuracy  : 15.35\n",
            "Epoch: 54,Batch : 750, Loss:0.7251142844557762\n",
            "Epoch: 54,Batch : 1500, Loss:0.7225515919923783\n",
            "validation loss 305.4737548828125\n",
            "testset Accuracy  : 15.75\n",
            "Epoch: 55,Batch : 750, Loss:0.7225439749956131\n",
            "Epoch: 55,Batch : 1500, Loss:0.723250226020813\n",
            "validation loss 304.8063659667969\n",
            "testset Accuracy  : 16.1\n",
            "Epoch: 56,Batch : 750, Loss:0.7223725637793541\n",
            "Epoch: 56,Batch : 1500, Loss:0.7225008317232132\n",
            "validation loss 305.226318359375\n",
            "testset Accuracy  : 15.45\n",
            "Epoch: 57,Batch : 750, Loss:0.7209217605590821\n",
            "Epoch: 57,Batch : 1500, Loss:0.7224905306100845\n",
            "validation loss 304.8291320800781\n",
            "testset Accuracy  : 15.9\n",
            "Epoch: 58,Batch : 750, Loss:0.7211570572853089\n",
            "Epoch: 58,Batch : 1500, Loss:0.7202838138341904\n",
            "validation loss 304.5658264160156\n",
            "testset Accuracy  : 16.05\n",
            "Epoch: 59,Batch : 750, Loss:0.7196023172140121\n",
            "Epoch: 59,Batch : 1500, Loss:0.7202416254878045\n",
            "validation loss 303.7220458984375\n",
            "testset Accuracy  : 18.15\n",
            "Epoch: 60,Batch : 750, Loss:0.7193755322098732\n",
            "Epoch: 60,Batch : 1500, Loss:0.7185241532325745\n",
            "validation loss 303.2715148925781\n",
            "testset Accuracy  : 16.1\n",
            "Epoch: 61,Batch : 750, Loss:0.7184599474072456\n",
            "Epoch: 61,Batch : 1500, Loss:0.719741381406784\n",
            "validation loss 303.7727966308594\n",
            "testset Accuracy  : 18.2\n",
            "Epoch: 62,Batch : 750, Loss:0.7173058806061745\n",
            "Epoch: 62,Batch : 1500, Loss:0.718415513753891\n",
            "validation loss 302.92266845703125\n",
            "testset Accuracy  : 19.3\n",
            "Epoch: 63,Batch : 750, Loss:0.7161962122321129\n",
            "Epoch: 63,Batch : 1500, Loss:0.7174335161447525\n",
            "validation loss 302.5906677246094\n",
            "testset Accuracy  : 18.3\n",
            "Epoch: 64,Batch : 750, Loss:0.7146476470828056\n",
            "Epoch: 64,Batch : 1500, Loss:0.7166413007974625\n",
            "validation loss 303.19415283203125\n",
            "testset Accuracy  : 18.55\n",
            "Epoch: 65,Batch : 750, Loss:0.7153685448765754\n",
            "Epoch: 65,Batch : 1500, Loss:0.715842519402504\n",
            "validation loss 302.02587890625\n",
            "testset Accuracy  : 17.05\n",
            "Epoch: 66,Batch : 750, Loss:0.7142205883860588\n",
            "Epoch: 66,Batch : 1500, Loss:0.7147525998353959\n",
            "validation loss 301.8207702636719\n",
            "testset Accuracy  : 17.1\n",
            "Epoch: 67,Batch : 750, Loss:0.7130376883745193\n",
            "Epoch: 67,Batch : 1500, Loss:0.7159320924878121\n",
            "validation loss 302.1271057128906\n",
            "testset Accuracy  : 16.85\n",
            "Epoch: 68,Batch : 750, Loss:0.7129131863117218\n",
            "Epoch: 68,Batch : 1500, Loss:0.7132924716472626\n",
            "validation loss 302.01995849609375\n",
            "testset Accuracy  : 17.45\n",
            "Epoch: 69,Batch : 750, Loss:0.7123663268089294\n",
            "Epoch: 69,Batch : 1500, Loss:0.7128394005298615\n",
            "validation loss 301.6576232910156\n",
            "testset Accuracy  : 17.4\n",
            "Epoch: 70,Batch : 750, Loss:0.7119143770337105\n",
            "Epoch: 70,Batch : 1500, Loss:0.713144705593586\n",
            "validation loss 302.1489562988281\n",
            "testset Accuracy  : 19.2\n",
            "Epoch: 71,Batch : 750, Loss:0.7117910604476929\n",
            "Epoch: 71,Batch : 1500, Loss:0.7112536133527756\n",
            "validation loss 301.1461181640625\n",
            "testset Accuracy  : 18.45\n",
            "Epoch: 72,Batch : 750, Loss:0.7107735773324967\n",
            "Epoch: 72,Batch : 1500, Loss:0.7104195922613143\n",
            "validation loss 300.83709716796875\n",
            "testset Accuracy  : 18.35\n",
            "Epoch: 73,Batch : 750, Loss:0.7092414910197258\n",
            "Epoch: 73,Batch : 1500, Loss:0.7112782626152039\n",
            "validation loss 300.77166748046875\n",
            "testset Accuracy  : 18.0\n",
            "Epoch: 74,Batch : 750, Loss:0.7089830603599548\n",
            "Epoch: 74,Batch : 1500, Loss:0.7102972904443741\n",
            "validation loss 300.9110107421875\n",
            "testset Accuracy  : 19.8\n",
            "Epoch: 75,Batch : 750, Loss:0.7092421269416809\n",
            "Epoch: 75,Batch : 1500, Loss:0.7091614668965339\n",
            "validation loss 300.4964599609375\n",
            "testset Accuracy  : 19.45\n",
            "Epoch: 76,Batch : 750, Loss:0.7077653671503067\n",
            "Epoch: 76,Batch : 1500, Loss:0.7090310073494911\n",
            "validation loss 301.4649963378906\n",
            "testset Accuracy  : 19.9\n",
            "Epoch: 77,Batch : 750, Loss:0.7084797866940499\n",
            "Epoch: 77,Batch : 1500, Loss:0.7072686128616333\n",
            "validation loss 300.650390625\n",
            "testset Accuracy  : 20.0\n",
            "Epoch: 78,Batch : 750, Loss:0.7071440513134003\n",
            "Epoch: 78,Batch : 1500, Loss:0.7071242854595184\n",
            "validation loss 299.960205078125\n",
            "testset Accuracy  : 19.95\n",
            "Epoch: 79,Batch : 750, Loss:0.7068804159164429\n",
            "Epoch: 79,Batch : 1500, Loss:0.7060971140861512\n",
            "validation loss 300.0814514160156\n",
            "testset Accuracy  : 20.35\n",
            "Epoch: 80,Batch : 750, Loss:0.706328600525856\n",
            "Epoch: 80,Batch : 1500, Loss:0.7051853204965591\n",
            "validation loss 300.4447937011719\n",
            "testset Accuracy  : 19.3\n",
            "Epoch: 81,Batch : 750, Loss:0.7061375406980515\n",
            "Epoch: 81,Batch : 1500, Loss:0.7047014632225037\n",
            "validation loss 299.1521911621094\n",
            "testset Accuracy  : 19.9\n",
            "Epoch: 82,Batch : 750, Loss:0.7044492337107658\n",
            "Epoch: 82,Batch : 1500, Loss:0.7060325609445572\n",
            "validation loss 299.3561096191406\n",
            "testset Accuracy  : 19.6\n",
            "Epoch: 83,Batch : 750, Loss:0.7057932026386261\n",
            "Epoch: 83,Batch : 1500, Loss:0.7036969902515411\n",
            "validation loss 299.2572326660156\n",
            "testset Accuracy  : 20.25\n",
            "Epoch: 84,Batch : 750, Loss:0.7027472602128982\n",
            "Epoch: 84,Batch : 1500, Loss:0.7050197784304619\n",
            "validation loss 299.1820983886719\n",
            "testset Accuracy  : 20.65\n",
            "Epoch: 85,Batch : 750, Loss:0.7043189265727997\n",
            "Epoch: 85,Batch : 1500, Loss:0.7025295267701149\n",
            "validation loss 298.4150390625\n",
            "testset Accuracy  : 20.45\n",
            "Epoch: 86,Batch : 750, Loss:0.7014607864022255\n",
            "Epoch: 86,Batch : 1500, Loss:0.7040751979351044\n",
            "validation loss 298.74407958984375\n",
            "testset Accuracy  : 19.9\n",
            "Epoch: 87,Batch : 750, Loss:0.7027189630270004\n",
            "Epoch: 87,Batch : 1500, Loss:0.7023388921618462\n",
            "validation loss 298.12884521484375\n",
            "testset Accuracy  : 20.25\n",
            "Epoch: 88,Batch : 750, Loss:0.7028686246871948\n",
            "Epoch: 88,Batch : 1500, Loss:0.7007020602822304\n",
            "validation loss 298.7784118652344\n",
            "testset Accuracy  : 20.3\n",
            "Epoch: 89,Batch : 750, Loss:0.7001884675621987\n",
            "Epoch: 89,Batch : 1500, Loss:0.7018655368089676\n",
            "validation loss 297.89691162109375\n",
            "testset Accuracy  : 20.25\n",
            "Epoch: 90,Batch : 750, Loss:0.6999792415499687\n",
            "Epoch: 90,Batch : 1500, Loss:0.7020602570176124\n",
            "validation loss 298.0300598144531\n",
            "testset Accuracy  : 21.25\n",
            "Epoch: 91,Batch : 750, Loss:0.6998940190672874\n",
            "Epoch: 91,Batch : 1500, Loss:0.7001377426981926\n",
            "validation loss 297.96319580078125\n",
            "testset Accuracy  : 22.7\n",
            "Epoch: 92,Batch : 750, Loss:0.6990026000142098\n",
            "Epoch: 92,Batch : 1500, Loss:0.7004595152139663\n",
            "validation loss 297.7839660644531\n",
            "testset Accuracy  : 20.35\n",
            "Epoch: 93,Batch : 750, Loss:0.6996522263288498\n",
            "Epoch: 93,Batch : 1500, Loss:0.6992273273468017\n",
            "validation loss 298.40240478515625\n",
            "testset Accuracy  : 22.0\n",
            "Epoch: 94,Batch : 750, Loss:0.6989278073906898\n",
            "Epoch: 94,Batch : 1500, Loss:0.6987036982178688\n",
            "validation loss 297.2251281738281\n",
            "testset Accuracy  : 21.3\n",
            "Epoch: 95,Batch : 750, Loss:0.697017145216465\n",
            "Epoch: 95,Batch : 1500, Loss:0.6990687528252602\n",
            "validation loss 297.7910461425781\n",
            "testset Accuracy  : 21.8\n",
            "Epoch: 96,Batch : 750, Loss:0.696573454797268\n",
            "Epoch: 96,Batch : 1500, Loss:0.6987790768146515\n",
            "validation loss 297.4310607910156\n",
            "testset Accuracy  : 22.25\n",
            "Epoch: 97,Batch : 750, Loss:0.6980880145430565\n",
            "Epoch: 97,Batch : 1500, Loss:0.6967096679210663\n",
            "validation loss 297.0979919433594\n",
            "testset Accuracy  : 21.65\n",
            "Epoch: 98,Batch : 750, Loss:0.697625708758831\n",
            "Epoch: 98,Batch : 1500, Loss:0.6967099151611328\n",
            "validation loss 296.99114990234375\n",
            "testset Accuracy  : 23.25\n",
            "Epoch: 99,Batch : 750, Loss:0.6966524258852005\n",
            "Epoch: 99,Batch : 1500, Loss:0.6965547712445259\n",
            "validation loss 296.92852783203125\n",
            "testset Accuracy  : 23.15\n",
            "Epoch: 100,Batch : 750, Loss:0.6949426706433296\n",
            "Epoch: 100,Batch : 1500, Loss:0.6971938046813011\n",
            "validation loss 297.4873352050781\n",
            "testset Accuracy  : 22.3\n",
            "Epoch: 101,Batch : 750, Loss:0.6953988595604896\n",
            "Epoch: 101,Batch : 1500, Loss:0.6957355667352676\n",
            "validation loss 296.7189636230469\n",
            "testset Accuracy  : 22.8\n",
            "Epoch: 102,Batch : 750, Loss:0.6944631250500679\n",
            "Epoch: 102,Batch : 1500, Loss:0.6958494558930397\n",
            "validation loss 297.26312255859375\n",
            "testset Accuracy  : 22.2\n",
            "Epoch: 103,Batch : 750, Loss:0.6930423926711082\n",
            "Epoch: 103,Batch : 1500, Loss:0.6958166208267212\n",
            "validation loss 297.2620544433594\n",
            "testset Accuracy  : 21.55\n",
            "Epoch: 104,Batch : 750, Loss:0.6941715279221534\n",
            "Epoch: 104,Batch : 1500, Loss:0.6949855591654778\n",
            "validation loss 296.2131652832031\n",
            "testset Accuracy  : 23.15\n",
            "Epoch: 105,Batch : 750, Loss:0.6931148993968964\n",
            "Epoch: 105,Batch : 1500, Loss:0.6949552498459816\n",
            "validation loss 296.60235595703125\n",
            "testset Accuracy  : 23.2\n",
            "Epoch: 106,Batch : 750, Loss:0.6922314092516899\n",
            "Epoch: 106,Batch : 1500, Loss:0.6942812485694885\n",
            "validation loss 296.6463623046875\n",
            "testset Accuracy  : 22.6\n",
            "Epoch: 107,Batch : 750, Loss:0.6933820018768311\n",
            "Epoch: 107,Batch : 1500, Loss:0.6930233398079872\n",
            "validation loss 295.962890625\n",
            "testset Accuracy  : 21.85\n",
            "Epoch: 108,Batch : 750, Loss:0.6932273529171944\n",
            "Epoch: 108,Batch : 1500, Loss:0.6917927728891373\n",
            "validation loss 296.1107177734375\n",
            "testset Accuracy  : 22.8\n",
            "Epoch: 109,Batch : 750, Loss:0.6920544711947441\n",
            "Epoch: 109,Batch : 1500, Loss:0.6932257316112518\n",
            "validation loss 295.5708312988281\n",
            "testset Accuracy  : 23.65\n",
            "Epoch: 110,Batch : 750, Loss:0.6918328515887261\n",
            "Epoch: 110,Batch : 1500, Loss:0.69250990909338\n",
            "validation loss 296.72686767578125\n",
            "testset Accuracy  : 22.65\n",
            "Epoch: 111,Batch : 750, Loss:0.6919821312427521\n",
            "Epoch: 111,Batch : 1500, Loss:0.6907613713741303\n",
            "validation loss 295.5771179199219\n",
            "testset Accuracy  : 22.0\n",
            "Epoch: 112,Batch : 750, Loss:0.6904213430285454\n",
            "Epoch: 112,Batch : 1500, Loss:0.6905929437279701\n",
            "validation loss 295.6210632324219\n",
            "testset Accuracy  : 21.25\n",
            "Epoch: 113,Batch : 750, Loss:0.6902326149940491\n",
            "Epoch: 113,Batch : 1500, Loss:0.6908651105165482\n",
            "validation loss 295.3650207519531\n",
            "testset Accuracy  : 22.7\n",
            "Epoch: 114,Batch : 750, Loss:0.6893000574707985\n",
            "Epoch: 114,Batch : 1500, Loss:0.6915270622968673\n",
            "validation loss 296.0242614746094\n",
            "testset Accuracy  : 23.35\n",
            "Epoch: 115,Batch : 750, Loss:0.6906999667882919\n",
            "Epoch: 115,Batch : 1500, Loss:0.6896077651977539\n",
            "validation loss 295.5474548339844\n",
            "testset Accuracy  : 23.2\n",
            "Epoch: 116,Batch : 750, Loss:0.6908418819904327\n",
            "Epoch: 116,Batch : 1500, Loss:0.6885298152565956\n",
            "validation loss 295.1981506347656\n",
            "testset Accuracy  : 22.9\n",
            "Epoch: 117,Batch : 750, Loss:0.689229127585888\n",
            "Epoch: 117,Batch : 1500, Loss:0.6899511414766312\n",
            "validation loss 295.41363525390625\n",
            "testset Accuracy  : 22.5\n",
            "Epoch: 118,Batch : 750, Loss:0.6880921053290368\n",
            "Epoch: 118,Batch : 1500, Loss:0.6894709329009057\n",
            "validation loss 295.75250244140625\n",
            "testset Accuracy  : 23.05\n",
            "Epoch: 119,Batch : 750, Loss:0.6864743264317512\n",
            "Epoch: 119,Batch : 1500, Loss:0.6888617233633995\n",
            "validation loss 296.0019836425781\n",
            "testset Accuracy  : 22.15\n",
            "Epoch: 120,Batch : 750, Loss:0.6865715601444244\n",
            "Epoch: 120,Batch : 1500, Loss:0.6889010247588158\n",
            "validation loss 295.46380615234375\n",
            "testset Accuracy  : 23.55\n",
            "Epoch: 121,Batch : 750, Loss:0.6854613251090049\n",
            "Epoch: 121,Batch : 1500, Loss:0.6891197574734688\n",
            "validation loss 295.4991760253906\n",
            "testset Accuracy  : 22.8\n",
            "Epoch: 122,Batch : 750, Loss:0.6863310111761093\n",
            "Epoch: 122,Batch : 1500, Loss:0.6875447714328766\n",
            "validation loss 295.15570068359375\n",
            "testset Accuracy  : 23.3\n",
            "Epoch: 123,Batch : 750, Loss:0.687414112150669\n",
            "Epoch: 123,Batch : 1500, Loss:0.6862225115299225\n",
            "validation loss 295.7864990234375\n",
            "testset Accuracy  : 23.3\n",
            "Epoch: 124,Batch : 750, Loss:0.686838565647602\n",
            "Epoch: 124,Batch : 1500, Loss:0.6853816694021225\n",
            "validation loss 294.9642333984375\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 125,Batch : 750, Loss:0.6857314235568046\n",
            "Epoch: 125,Batch : 1500, Loss:0.6863641726970673\n",
            "validation loss 294.6396789550781\n",
            "testset Accuracy  : 22.95\n",
            "Epoch: 126,Batch : 750, Loss:0.6841073611974716\n",
            "Epoch: 126,Batch : 1500, Loss:0.6877642855048179\n",
            "validation loss 295.39306640625\n",
            "testset Accuracy  : 22.7\n",
            "Epoch: 127,Batch : 750, Loss:0.6838989233970643\n",
            "Epoch: 127,Batch : 1500, Loss:0.6865489381551743\n",
            "validation loss 295.50299072265625\n",
            "testset Accuracy  : 23.35\n",
            "Epoch: 128,Batch : 750, Loss:0.6847111156582832\n",
            "Epoch: 128,Batch : 1500, Loss:0.6844884304404258\n",
            "validation loss 295.04302978515625\n",
            "testset Accuracy  : 22.85\n",
            "Epoch: 129,Batch : 750, Loss:0.6845439128279686\n",
            "Epoch: 129,Batch : 1500, Loss:0.684489660859108\n",
            "validation loss 295.169921875\n",
            "testset Accuracy  : 22.55\n",
            "Epoch: 130,Batch : 750, Loss:0.683272216796875\n",
            "Epoch: 130,Batch : 1500, Loss:0.6850813198089599\n",
            "validation loss 294.4765625\n",
            "testset Accuracy  : 22.65\n",
            "Epoch: 131,Batch : 750, Loss:0.6837705984115601\n",
            "Epoch: 131,Batch : 1500, Loss:0.6840258626937866\n",
            "validation loss 294.5940246582031\n",
            "testset Accuracy  : 22.4\n",
            "Epoch: 132,Batch : 750, Loss:0.6829174444079399\n",
            "Epoch: 132,Batch : 1500, Loss:0.6841089022755623\n",
            "validation loss 294.34503173828125\n",
            "testset Accuracy  : 21.95\n",
            "Epoch: 133,Batch : 750, Loss:0.6823972356915474\n",
            "Epoch: 133,Batch : 1500, Loss:0.6845192503929138\n",
            "validation loss 294.71807861328125\n",
            "testset Accuracy  : 24.65\n",
            "Epoch: 134,Batch : 750, Loss:0.6821228941679001\n",
            "Epoch: 134,Batch : 1500, Loss:0.6837381235957146\n",
            "validation loss 294.6575012207031\n",
            "testset Accuracy  : 24.35\n",
            "Epoch: 135,Batch : 750, Loss:0.6820209214687347\n",
            "Epoch: 135,Batch : 1500, Loss:0.6833997284770011\n",
            "validation loss 294.05804443359375\n",
            "testset Accuracy  : 23.25\n",
            "Epoch: 136,Batch : 750, Loss:0.6809403240680695\n",
            "Epoch: 136,Batch : 1500, Loss:0.6824529859423637\n",
            "validation loss 294.0506591796875\n",
            "testset Accuracy  : 23.75\n",
            "Epoch: 137,Batch : 750, Loss:0.6825640801191329\n",
            "Epoch: 137,Batch : 1500, Loss:0.6818439847826958\n",
            "validation loss 294.0050964355469\n",
            "testset Accuracy  : 23.75\n",
            "Epoch: 138,Batch : 750, Loss:0.6811616593003273\n",
            "Epoch: 138,Batch : 1500, Loss:0.6824739081859589\n",
            "validation loss 294.6632080078125\n",
            "testset Accuracy  : 21.9\n",
            "Epoch: 139,Batch : 750, Loss:0.681932762503624\n",
            "Epoch: 139,Batch : 1500, Loss:0.680648324906826\n",
            "validation loss 294.18206787109375\n",
            "testset Accuracy  : 23.6\n",
            "Epoch: 140,Batch : 750, Loss:0.6806376029253006\n",
            "Epoch: 140,Batch : 1500, Loss:0.6816484994292259\n",
            "validation loss 294.43359375\n",
            "testset Accuracy  : 23.0\n",
            "Epoch: 141,Batch : 750, Loss:0.6812383124232292\n",
            "Epoch: 141,Batch : 1500, Loss:0.6801779537200928\n",
            "validation loss 293.9521789550781\n",
            "testset Accuracy  : 23.25\n",
            "Epoch: 142,Batch : 750, Loss:0.6780976505875588\n",
            "Epoch: 142,Batch : 1500, Loss:0.6822249178886414\n",
            "validation loss 294.3075256347656\n",
            "testset Accuracy  : 23.85\n",
            "Epoch: 143,Batch : 750, Loss:0.6798699781298637\n",
            "Epoch: 143,Batch : 1500, Loss:0.6804337977170944\n",
            "validation loss 293.7229919433594\n",
            "testset Accuracy  : 23.3\n",
            "Epoch: 144,Batch : 750, Loss:0.6787545666098594\n",
            "Epoch: 144,Batch : 1500, Loss:0.6808816229104996\n",
            "validation loss 295.09100341796875\n",
            "testset Accuracy  : 22.9\n",
            "Epoch: 145,Batch : 750, Loss:0.679613909125328\n",
            "Epoch: 145,Batch : 1500, Loss:0.6793405439257622\n",
            "validation loss 294.0302734375\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 146,Batch : 750, Loss:0.6802436257600785\n",
            "Epoch: 146,Batch : 1500, Loss:0.6790525757074356\n",
            "validation loss 294.05499267578125\n",
            "testset Accuracy  : 23.2\n",
            "Epoch: 147,Batch : 750, Loss:0.6792048724293709\n",
            "Epoch: 147,Batch : 1500, Loss:0.6792168311476707\n",
            "validation loss 294.6359558105469\n",
            "testset Accuracy  : 23.6\n",
            "Epoch: 148,Batch : 750, Loss:0.6772920185923577\n",
            "Epoch: 148,Batch : 1500, Loss:0.6787145898342133\n",
            "validation loss 294.0163269042969\n",
            "testset Accuracy  : 23.3\n",
            "Epoch: 149,Batch : 750, Loss:0.6780805880427361\n",
            "Epoch: 149,Batch : 1500, Loss:0.6783515250086785\n",
            "validation loss 293.5827941894531\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 150,Batch : 750, Loss:0.6782643864154816\n",
            "Epoch: 150,Batch : 1500, Loss:0.6787349947094917\n",
            "validation loss 293.63519287109375\n",
            "testset Accuracy  : 22.7\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50,150):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_loss = 0.0\n",
        "    for k, data1 in enumerate(vaild_loader, 0):\n",
        "      val_inputs, val_label = data1[0].to(device), data1[1].to(device)\n",
        "      val_output = net(val_inputs)\n",
        "      v_loss = criterion(val_output, val_label)\n",
        "      val_loss += v_loss\n",
        "  print(\"validation loss {}\".format(val_loss))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      images, labels = data[0].to(device), data[1].to(device)\n",
        "      outputs= net(images)\n",
        "      _, predicted = torch.max(outputs.data,1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(\"testset Accuracy  : {}\".format(100* correct/total))\n",
        "  total=0\n",
        "  correct=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDN8WbLabRhN",
        "outputId": "46905d1b-6013-43ad-f8e9-f993a39aca6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 151,Batch : 1500, Loss:1.356152800142765\n",
            "testset Accuracy  : 24.55\n",
            "Epoch: 152,Batch : 1500, Loss:1.354506415605545\n",
            "Epoch: 153,Batch : 1500, Loss:1.353373305439949\n",
            "Epoch: 154,Batch : 1500, Loss:1.3540266038179398\n",
            "Epoch: 155,Batch : 1500, Loss:1.3525650131106377\n",
            "Epoch: 156,Batch : 1500, Loss:1.3522066872119904\n",
            "Epoch: 157,Batch : 1500, Loss:1.3524565351605415\n",
            "Epoch: 158,Batch : 1500, Loss:1.3511219604611397\n",
            "Epoch: 159,Batch : 1500, Loss:1.351419348180294\n",
            "Epoch: 160,Batch : 1500, Loss:1.3506899450421332\n",
            "Epoch: 161,Batch : 1500, Loss:1.3497425371408462\n",
            "testset Accuracy  : 22.9\n",
            "Epoch: 162,Batch : 1500, Loss:1.3496876139640808\n",
            "Epoch: 163,Batch : 1500, Loss:1.3486166974902154\n",
            "Epoch: 164,Batch : 1500, Loss:1.348358838737011\n",
            "Epoch: 165,Batch : 1500, Loss:1.3475145441293717\n",
            "Epoch: 166,Batch : 1500, Loss:1.3477143142819406\n",
            "Epoch: 167,Batch : 1500, Loss:1.3456502810716628\n",
            "Epoch: 168,Batch : 1500, Loss:1.3450240722298623\n",
            "Epoch: 169,Batch : 1500, Loss:1.344456161737442\n",
            "Epoch: 170,Batch : 1500, Loss:1.3456972991228104\n",
            "Epoch: 171,Batch : 1500, Loss:1.3436535044312476\n",
            "testset Accuracy  : 23.85\n",
            "Epoch: 172,Batch : 1500, Loss:1.3430515196323394\n",
            "Epoch: 173,Batch : 1500, Loss:1.3433785270452498\n",
            "Epoch: 174,Batch : 1500, Loss:1.342325342774391\n",
            "Epoch: 175,Batch : 1500, Loss:1.3425165112018584\n",
            "Epoch: 176,Batch : 1500, Loss:1.3419902868270874\n",
            "Epoch: 177,Batch : 1500, Loss:1.3417212458848953\n",
            "Epoch: 178,Batch : 1500, Loss:1.340546258032322\n",
            "Epoch: 179,Batch : 1500, Loss:1.3404320673942567\n",
            "Epoch: 180,Batch : 1500, Loss:1.3395965083837509\n",
            "Epoch: 181,Batch : 1500, Loss:1.3399133703708648\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 182,Batch : 1500, Loss:1.3389858967661858\n",
            "Epoch: 183,Batch : 1500, Loss:1.3377396706342697\n",
            "Epoch: 184,Batch : 1500, Loss:1.3381727501749991\n",
            "Epoch: 185,Batch : 1500, Loss:1.3373549867272376\n",
            "Epoch: 186,Batch : 1500, Loss:1.3368141505718232\n",
            "Epoch: 187,Batch : 1500, Loss:1.336412567794323\n",
            "Epoch: 188,Batch : 1500, Loss:1.3363611749410629\n",
            "Epoch: 189,Batch : 1500, Loss:1.3361433888077736\n",
            "Epoch: 190,Batch : 1500, Loss:1.3351669732928275\n",
            "Epoch: 191,Batch : 1500, Loss:1.3353238626122474\n",
            "testset Accuracy  : 25.1\n",
            "Epoch: 192,Batch : 1500, Loss:1.3340859696269036\n",
            "Epoch: 193,Batch : 1500, Loss:1.3345753029584884\n",
            "Epoch: 194,Batch : 1500, Loss:1.3327437722086906\n",
            "Epoch: 195,Batch : 1500, Loss:1.3325648748278618\n",
            "Epoch: 196,Batch : 1500, Loss:1.33292761605978\n",
            "Epoch: 197,Batch : 1500, Loss:1.3326265065073968\n",
            "Epoch: 198,Batch : 1500, Loss:1.332198729991913\n",
            "Epoch: 199,Batch : 1500, Loss:1.3307768905162811\n",
            "Epoch: 200,Batch : 1500, Loss:1.3316577304601669\n",
            "Epoch: 201,Batch : 1500, Loss:1.3312681092619896\n",
            "testset Accuracy  : 23.55\n",
            "Epoch: 202,Batch : 1500, Loss:1.3300718283057213\n",
            "Epoch: 203,Batch : 1500, Loss:1.328756722509861\n",
            "Epoch: 204,Batch : 1500, Loss:1.3289801535606385\n",
            "Epoch: 205,Batch : 1500, Loss:1.3290909996628761\n",
            "Epoch: 206,Batch : 1500, Loss:1.3288282492756844\n",
            "Epoch: 207,Batch : 1500, Loss:1.3259116994738578\n",
            "Epoch: 208,Batch : 1500, Loss:1.327648452103138\n",
            "Epoch: 209,Batch : 1500, Loss:1.3261916366815567\n",
            "Epoch: 210,Batch : 1500, Loss:1.3259422141313553\n",
            "Epoch: 211,Batch : 1500, Loss:1.3272867572903633\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 212,Batch : 1500, Loss:1.3250009799599647\n",
            "Epoch: 213,Batch : 1500, Loss:1.325742245554924\n",
            "Epoch: 214,Batch : 1500, Loss:1.324875267148018\n",
            "Epoch: 215,Batch : 1500, Loss:1.3257945635914803\n",
            "Epoch: 216,Batch : 1500, Loss:1.3247394349575043\n",
            "Epoch: 217,Batch : 1500, Loss:1.3242803882956504\n",
            "Epoch: 218,Batch : 1500, Loss:1.324875587105751\n",
            "Epoch: 219,Batch : 1500, Loss:1.3228302406668664\n",
            "Epoch: 220,Batch : 1500, Loss:1.323821478188038\n",
            "Epoch: 221,Batch : 1500, Loss:1.3215726226568223\n",
            "testset Accuracy  : 25.05\n",
            "Epoch: 222,Batch : 1500, Loss:1.3219034386873245\n",
            "Epoch: 223,Batch : 1500, Loss:1.321890009045601\n",
            "Epoch: 224,Batch : 1500, Loss:1.3214131680727006\n",
            "Epoch: 225,Batch : 1500, Loss:1.3201057891845702\n",
            "Epoch: 226,Batch : 1500, Loss:1.3203487731218337\n",
            "Epoch: 227,Batch : 1500, Loss:1.3206586668491362\n",
            "Epoch: 228,Batch : 1500, Loss:1.3200249549746514\n",
            "Epoch: 229,Batch : 1500, Loss:1.3189300091266631\n",
            "Epoch: 230,Batch : 1500, Loss:1.3183127276301383\n",
            "Epoch: 231,Batch : 1500, Loss:1.3190647971630096\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 232,Batch : 1500, Loss:1.3180151992440223\n",
            "Epoch: 233,Batch : 1500, Loss:1.3177414385676385\n",
            "Epoch: 234,Batch : 1500, Loss:1.316879695892334\n",
            "Epoch: 235,Batch : 1500, Loss:1.3184614496827125\n",
            "Epoch: 236,Batch : 1500, Loss:1.3158302544355391\n",
            "Epoch: 237,Batch : 1500, Loss:1.3167031082510947\n",
            "Epoch: 238,Batch : 1500, Loss:1.3158412056565285\n",
            "Epoch: 239,Batch : 1500, Loss:1.3145748578310013\n",
            "Epoch: 240,Batch : 1500, Loss:1.3158343454003334\n",
            "Epoch: 241,Batch : 1500, Loss:1.3157863134741783\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 242,Batch : 1500, Loss:1.3155019996166228\n",
            "Epoch: 243,Batch : 1500, Loss:1.3147938547730447\n",
            "Epoch: 244,Batch : 1500, Loss:1.3132602699995042\n",
            "Epoch: 245,Batch : 1500, Loss:1.3139528390169144\n",
            "Epoch: 246,Batch : 1500, Loss:1.3138604764938355\n",
            "Epoch: 247,Batch : 1500, Loss:1.313586787879467\n",
            "Epoch: 248,Batch : 1500, Loss:1.3119021624922753\n",
            "Epoch: 249,Batch : 1500, Loss:1.3118122652173043\n",
            "Epoch: 250,Batch : 1500, Loss:1.3118606874346732\n",
            "Epoch: 251,Batch : 1500, Loss:1.3117450519800187\n",
            "testset Accuracy  : 24.0\n",
            "Epoch: 252,Batch : 1500, Loss:1.3128329893946649\n",
            "Epoch: 253,Batch : 1500, Loss:1.3111004486083984\n",
            "Epoch: 254,Batch : 1500, Loss:1.3101472645401955\n",
            "Epoch: 255,Batch : 1500, Loss:1.3108889645934105\n",
            "Epoch: 256,Batch : 1500, Loss:1.3104990471601485\n",
            "Epoch: 257,Batch : 1500, Loss:1.3089838682413102\n",
            "Epoch: 258,Batch : 1500, Loss:1.3106234385371207\n",
            "Epoch: 259,Batch : 1500, Loss:1.3089077485203744\n",
            "Epoch: 260,Batch : 1500, Loss:1.308598645210266\n",
            "Epoch: 261,Batch : 1500, Loss:1.308258062362671\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 262,Batch : 1500, Loss:1.3096295354962348\n",
            "Epoch: 263,Batch : 1500, Loss:1.3080728919506073\n",
            "Epoch: 264,Batch : 1500, Loss:1.3075539435744286\n",
            "Epoch: 265,Batch : 1500, Loss:1.3079703119397164\n",
            "Epoch: 266,Batch : 1500, Loss:1.307250615656376\n",
            "Epoch: 267,Batch : 1500, Loss:1.3072919015884399\n",
            "Epoch: 268,Batch : 1500, Loss:1.3070282329320908\n",
            "Epoch: 269,Batch : 1500, Loss:1.3062758795619012\n",
            "Epoch: 270,Batch : 1500, Loss:1.3063515499830245\n",
            "Epoch: 271,Batch : 1500, Loss:1.3054354922771454\n",
            "testset Accuracy  : 24.75\n",
            "Epoch: 272,Batch : 1500, Loss:1.3043602904081344\n",
            "Epoch: 273,Batch : 1500, Loss:1.305544188797474\n",
            "Epoch: 274,Batch : 1500, Loss:1.3058884178400039\n",
            "Epoch: 275,Batch : 1500, Loss:1.3042817342877389\n",
            "Epoch: 276,Batch : 1500, Loss:1.3054578736424447\n",
            "Epoch: 277,Batch : 1500, Loss:1.3026658313274384\n",
            "Epoch: 278,Batch : 1500, Loss:1.3047736374735832\n",
            "Epoch: 279,Batch : 1500, Loss:1.303537789106369\n",
            "Epoch: 280,Batch : 1500, Loss:1.3039330312609672\n",
            "Epoch: 281,Batch : 1500, Loss:1.3041364357471465\n",
            "testset Accuracy  : 24.0\n",
            "Epoch: 282,Batch : 1500, Loss:1.3034187496900558\n",
            "Epoch: 283,Batch : 1500, Loss:1.3011271298527718\n",
            "Epoch: 284,Batch : 1500, Loss:1.3008565819263458\n",
            "Epoch: 285,Batch : 1500, Loss:1.301658398449421\n",
            "Epoch: 286,Batch : 1500, Loss:1.3010690321922302\n",
            "Epoch: 287,Batch : 1500, Loss:1.3013782144784927\n",
            "Epoch: 288,Batch : 1500, Loss:1.3018006491065026\n",
            "Epoch: 289,Batch : 1500, Loss:1.3008823046684266\n",
            "Epoch: 290,Batch : 1500, Loss:1.3001278159022331\n",
            "Epoch: 291,Batch : 1500, Loss:1.3004484485387802\n",
            "testset Accuracy  : 24.15\n",
            "Epoch: 292,Batch : 1500, Loss:1.301006957769394\n",
            "Epoch: 293,Batch : 1500, Loss:1.2998470733761787\n",
            "Epoch: 294,Batch : 1500, Loss:1.3006052970290185\n",
            "Epoch: 295,Batch : 1500, Loss:1.2993043358922005\n",
            "Epoch: 296,Batch : 1500, Loss:1.3000450486540793\n",
            "Epoch: 297,Batch : 1500, Loss:1.2995801627635957\n",
            "Epoch: 298,Batch : 1500, Loss:1.2996196612119675\n",
            "Epoch: 299,Batch : 1500, Loss:1.2974032635688781\n",
            "Epoch: 300,Batch : 1500, Loss:1.2988356348276138\n",
            "Epoch: 301,Batch : 1500, Loss:1.2972560892701148\n",
            "testset Accuracy  : 24.0\n",
            "Epoch: 302,Batch : 1500, Loss:1.2973705099225044\n",
            "Epoch: 303,Batch : 1500, Loss:1.2974479253292084\n",
            "Epoch: 304,Batch : 1500, Loss:1.2974503111839295\n",
            "Epoch: 305,Batch : 1500, Loss:1.2967139762043953\n",
            "Epoch: 306,Batch : 1500, Loss:1.296757108926773\n",
            "Epoch: 307,Batch : 1500, Loss:1.2969958367943764\n",
            "Epoch: 308,Batch : 1500, Loss:1.2959721888303757\n",
            "Epoch: 309,Batch : 1500, Loss:1.2964592919945717\n",
            "Epoch: 310,Batch : 1500, Loss:1.2958867419362068\n",
            "Epoch: 311,Batch : 1500, Loss:1.2956849017739296\n",
            "testset Accuracy  : 23.4\n",
            "Epoch: 312,Batch : 1500, Loss:1.2959435952305793\n",
            "Epoch: 313,Batch : 1500, Loss:1.2964011110067368\n",
            "Epoch: 314,Batch : 1500, Loss:1.2947759408950805\n",
            "Epoch: 315,Batch : 1500, Loss:1.2936845020651817\n",
            "Epoch: 316,Batch : 1500, Loss:1.294757481753826\n",
            "Epoch: 317,Batch : 1500, Loss:1.294542943239212\n",
            "Epoch: 318,Batch : 1500, Loss:1.2944940664172173\n",
            "Epoch: 319,Batch : 1500, Loss:1.293282651901245\n",
            "Epoch: 320,Batch : 1500, Loss:1.2940626819133758\n",
            "Epoch: 321,Batch : 1500, Loss:1.2929854403734207\n",
            "testset Accuracy  : 23.85\n",
            "Epoch: 322,Batch : 1500, Loss:1.293371536374092\n",
            "Epoch: 323,Batch : 1500, Loss:1.2926562737226486\n",
            "Epoch: 324,Batch : 1500, Loss:1.2935302961468698\n",
            "Epoch: 325,Batch : 1500, Loss:1.2925872408151626\n",
            "Epoch: 326,Batch : 1500, Loss:1.2928344928026199\n",
            "Epoch: 327,Batch : 1500, Loss:1.2919877673387528\n",
            "Epoch: 328,Batch : 1500, Loss:1.2921208340525627\n",
            "Epoch: 329,Batch : 1500, Loss:1.2926060763001441\n",
            "Epoch: 330,Batch : 1500, Loss:1.2904641468524933\n",
            "Epoch: 331,Batch : 1500, Loss:1.2926629962921143\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 332,Batch : 1500, Loss:1.2914301013946534\n",
            "Epoch: 333,Batch : 1500, Loss:1.2913468605875968\n",
            "Epoch: 334,Batch : 1500, Loss:1.2905235562324524\n",
            "Epoch: 335,Batch : 1500, Loss:1.2907113624215125\n",
            "Epoch: 336,Batch : 1500, Loss:1.2904159627556802\n",
            "Epoch: 337,Batch : 1500, Loss:1.2909168130755424\n",
            "Epoch: 338,Batch : 1500, Loss:1.2904906568527221\n",
            "Epoch: 339,Batch : 1500, Loss:1.2898924285769462\n",
            "Epoch: 340,Batch : 1500, Loss:1.28968569535017\n",
            "Epoch: 341,Batch : 1500, Loss:1.2898438979387283\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 342,Batch : 1500, Loss:1.289171200811863\n",
            "Epoch: 343,Batch : 1500, Loss:1.2890803292393684\n",
            "Epoch: 344,Batch : 1500, Loss:1.289056000828743\n",
            "Epoch: 345,Batch : 1500, Loss:1.2897552130818366\n",
            "Epoch: 346,Batch : 1500, Loss:1.2909340510964393\n",
            "Epoch: 347,Batch : 1500, Loss:1.2883755356073379\n",
            "Epoch: 348,Batch : 1500, Loss:1.2888410305380822\n",
            "Epoch: 349,Batch : 1500, Loss:1.2889183691740036\n",
            "Epoch: 350,Batch : 1500, Loss:1.2888769978284835\n",
            "Epoch: 351,Batch : 1500, Loss:1.2881051893234252\n",
            "testset Accuracy  : 24.0\n",
            "Epoch: 352,Batch : 1500, Loss:1.2871856706142426\n",
            "Epoch: 353,Batch : 1500, Loss:1.28714565718174\n",
            "Epoch: 354,Batch : 1500, Loss:1.2878258607387543\n",
            "Epoch: 355,Batch : 1500, Loss:1.2877528196573258\n",
            "Epoch: 356,Batch : 1500, Loss:1.2871122712492942\n",
            "Epoch: 357,Batch : 1500, Loss:1.2874465299248696\n",
            "Epoch: 358,Batch : 1500, Loss:1.2865736521482467\n",
            "Epoch: 359,Batch : 1500, Loss:1.2862535738348961\n",
            "Epoch: 360,Batch : 1500, Loss:1.287717581987381\n",
            "Epoch: 361,Batch : 1500, Loss:1.287064438521862\n",
            "testset Accuracy  : 23.9\n",
            "Epoch: 362,Batch : 1500, Loss:1.286762172460556\n",
            "Epoch: 363,Batch : 1500, Loss:1.287299494445324\n",
            "Epoch: 364,Batch : 1500, Loss:1.286213527083397\n",
            "Epoch: 365,Batch : 1500, Loss:1.2862504673600197\n",
            "Epoch: 366,Batch : 1500, Loss:1.2869077506661415\n",
            "Epoch: 367,Batch : 1500, Loss:1.2862857360243798\n",
            "Epoch: 368,Batch : 1500, Loss:1.2863416219949722\n",
            "Epoch: 369,Batch : 1500, Loss:1.2868106105327606\n",
            "Epoch: 370,Batch : 1500, Loss:1.2850633496046067\n",
            "Epoch: 371,Batch : 1500, Loss:1.2839353039264678\n",
            "testset Accuracy  : 24.5\n",
            "Epoch: 372,Batch : 1500, Loss:1.2849738936424255\n",
            "Epoch: 373,Batch : 1500, Loss:1.284526262998581\n",
            "Epoch: 374,Batch : 1500, Loss:1.285126894414425\n",
            "Epoch: 375,Batch : 1500, Loss:1.2863903509378434\n",
            "Epoch: 376,Batch : 1500, Loss:1.283259883105755\n",
            "Epoch: 377,Batch : 1500, Loss:1.283735613465309\n",
            "Epoch: 378,Batch : 1500, Loss:1.284887147128582\n",
            "Epoch: 379,Batch : 1500, Loss:1.2843293040990829\n",
            "Epoch: 380,Batch : 1500, Loss:1.284400042295456\n",
            "Epoch: 381,Batch : 1500, Loss:1.2836904668211937\n",
            "testset Accuracy  : 23.8\n",
            "Epoch: 382,Batch : 1500, Loss:1.284359810054302\n",
            "Epoch: 383,Batch : 1500, Loss:1.2828885459899901\n",
            "Epoch: 384,Batch : 1500, Loss:1.2842786386013032\n",
            "Epoch: 385,Batch : 1500, Loss:1.2826565753817558\n",
            "Epoch: 386,Batch : 1500, Loss:1.2833268176317214\n",
            "Epoch: 387,Batch : 1500, Loss:1.2829010375738144\n",
            "Epoch: 388,Batch : 1500, Loss:1.2829092000126838\n",
            "Epoch: 389,Batch : 1500, Loss:1.2828977674245834\n",
            "Epoch: 390,Batch : 1500, Loss:1.2826771354079247\n",
            "Epoch: 391,Batch : 1500, Loss:1.2825794144868852\n",
            "testset Accuracy  : 24.5\n",
            "Epoch: 392,Batch : 1500, Loss:1.2826675944924355\n",
            "Epoch: 393,Batch : 1500, Loss:1.2822796164751054\n",
            "Epoch: 394,Batch : 1500, Loss:1.2821360312104224\n",
            "Epoch: 395,Batch : 1500, Loss:1.2823790541887283\n",
            "Epoch: 396,Batch : 1500, Loss:1.2816874329447747\n",
            "Epoch: 397,Batch : 1500, Loss:1.2821018618941307\n",
            "Epoch: 398,Batch : 1500, Loss:1.2821677022576332\n",
            "Epoch: 399,Batch : 1500, Loss:1.2818813940882683\n",
            "Epoch: 400,Batch : 1500, Loss:1.2813810901641847\n",
            "Epoch: 401,Batch : 1500, Loss:1.2793583384752274\n",
            "testset Accuracy  : 24.3\n",
            "Epoch: 402,Batch : 1500, Loss:1.2811395552754403\n",
            "Epoch: 403,Batch : 1500, Loss:1.2810493406653405\n",
            "Epoch: 404,Batch : 1500, Loss:1.2804836949706078\n",
            "Epoch: 405,Batch : 1500, Loss:1.2804142208695413\n",
            "Epoch: 406,Batch : 1500, Loss:1.2805225655436516\n",
            "Epoch: 407,Batch : 1500, Loss:1.280794133245945\n",
            "Epoch: 408,Batch : 1500, Loss:1.2805209537744522\n",
            "Epoch: 409,Batch : 1500, Loss:1.2809036886096001\n",
            "Epoch: 410,Batch : 1500, Loss:1.2796754359602929\n",
            "Epoch: 411,Batch : 1500, Loss:1.2815238172411918\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 412,Batch : 1500, Loss:1.2800487304925918\n",
            "Epoch: 413,Batch : 1500, Loss:1.2810613084435463\n",
            "Epoch: 414,Batch : 1500, Loss:1.27948355448246\n",
            "Epoch: 415,Batch : 1500, Loss:1.280319626748562\n",
            "Epoch: 416,Batch : 1500, Loss:1.2793324013352394\n",
            "Epoch: 417,Batch : 1500, Loss:1.278379777789116\n",
            "Epoch: 418,Batch : 1500, Loss:1.2792324370741843\n",
            "Epoch: 419,Batch : 1500, Loss:1.279275942325592\n",
            "Epoch: 420,Batch : 1500, Loss:1.2795703744888305\n",
            "Epoch: 421,Batch : 1500, Loss:1.2790547375679016\n",
            "testset Accuracy  : 23.95\n",
            "Epoch: 422,Batch : 1500, Loss:1.2792878316044807\n",
            "Epoch: 423,Batch : 1500, Loss:1.2796976158618927\n",
            "Epoch: 424,Batch : 1500, Loss:1.2801767945885658\n",
            "Epoch: 425,Batch : 1500, Loss:1.279167283654213\n",
            "Epoch: 426,Batch : 1500, Loss:1.2786226916909218\n",
            "Epoch: 427,Batch : 1500, Loss:1.2785640316009521\n",
            "Epoch: 428,Batch : 1500, Loss:1.2779976459145546\n",
            "Epoch: 429,Batch : 1500, Loss:1.2792241445183754\n",
            "Epoch: 430,Batch : 1500, Loss:1.2782913889884948\n",
            "Epoch: 431,Batch : 1500, Loss:1.2785782226920128\n",
            "testset Accuracy  : 24.15\n",
            "Epoch: 432,Batch : 1500, Loss:1.2787388072609902\n",
            "Epoch: 433,Batch : 1500, Loss:1.2780597386956214\n",
            "Epoch: 434,Batch : 1500, Loss:1.2775385763049125\n",
            "Epoch: 435,Batch : 1500, Loss:1.2774202042222023\n",
            "Epoch: 436,Batch : 1500, Loss:1.2775546466708183\n",
            "Epoch: 437,Batch : 1500, Loss:1.278689102113247\n",
            "Epoch: 438,Batch : 1500, Loss:1.2775024703741074\n",
            "Epoch: 439,Batch : 1500, Loss:1.2769422565102577\n",
            "Epoch: 440,Batch : 1500, Loss:1.277117868721485\n",
            "Epoch: 441,Batch : 1500, Loss:1.2772833052873611\n",
            "testset Accuracy  : 23.9\n",
            "Epoch: 442,Batch : 1500, Loss:1.2787992913126947\n",
            "Epoch: 443,Batch : 1500, Loss:1.277126498758793\n",
            "Epoch: 444,Batch : 1500, Loss:1.2782030955553054\n",
            "Epoch: 445,Batch : 1500, Loss:1.2782440403699875\n",
            "Epoch: 446,Batch : 1500, Loss:1.2774217731952666\n",
            "Epoch: 447,Batch : 1500, Loss:1.2769800337553023\n",
            "Epoch: 448,Batch : 1500, Loss:1.276871612071991\n",
            "Epoch: 449,Batch : 1500, Loss:1.2772845959067345\n",
            "Epoch: 450,Batch : 1500, Loss:1.2759256453514098\n",
            "Epoch: 451,Batch : 1500, Loss:1.276736763894558\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 452,Batch : 1500, Loss:1.2754244522452354\n",
            "Epoch: 453,Batch : 1500, Loss:1.277852609872818\n",
            "Epoch: 454,Batch : 1500, Loss:1.2759692433476448\n",
            "Epoch: 455,Batch : 1500, Loss:1.2762604345679283\n",
            "Epoch: 456,Batch : 1500, Loss:1.2770299645662309\n",
            "Epoch: 457,Batch : 1500, Loss:1.2771935742497444\n",
            "Epoch: 458,Batch : 1500, Loss:1.2758969853520394\n",
            "Epoch: 459,Batch : 1500, Loss:1.2763803279399872\n",
            "Epoch: 460,Batch : 1500, Loss:1.2763774191737176\n",
            "Epoch: 461,Batch : 1500, Loss:1.2760421830415725\n",
            "testset Accuracy  : 24.3\n",
            "Epoch: 462,Batch : 1500, Loss:1.2758362016677856\n",
            "Epoch: 463,Batch : 1500, Loss:1.2772574859261512\n",
            "Epoch: 464,Batch : 1500, Loss:1.2762361882328987\n",
            "Epoch: 465,Batch : 1500, Loss:1.275178246676922\n",
            "Epoch: 466,Batch : 1500, Loss:1.2759488254785538\n",
            "Epoch: 467,Batch : 1500, Loss:1.2756526024341583\n",
            "Epoch: 468,Batch : 1500, Loss:1.2750659831762314\n",
            "Epoch: 469,Batch : 1500, Loss:1.2759741573929786\n",
            "Epoch: 470,Batch : 1500, Loss:1.2755225781202317\n",
            "Epoch: 471,Batch : 1500, Loss:1.2765375195145607\n",
            "testset Accuracy  : 23.85\n",
            "Epoch: 472,Batch : 1500, Loss:1.275550703048706\n",
            "Epoch: 473,Batch : 1500, Loss:1.2763595588803291\n",
            "Epoch: 474,Batch : 1500, Loss:1.2751388051509858\n",
            "Epoch: 475,Batch : 1500, Loss:1.2775208835601806\n",
            "Epoch: 476,Batch : 1500, Loss:1.275553177535534\n",
            "Epoch: 477,Batch : 1500, Loss:1.2743251671791076\n",
            "Epoch: 478,Batch : 1500, Loss:1.2755891969799995\n",
            "Epoch: 479,Batch : 1500, Loss:1.2752230630517005\n",
            "Epoch: 480,Batch : 1500, Loss:1.2758605294823646\n",
            "Epoch: 481,Batch : 1500, Loss:1.273740081369877\n",
            "testset Accuracy  : 24.0\n",
            "Epoch: 482,Batch : 1500, Loss:1.2742452670335769\n",
            "Epoch: 483,Batch : 1500, Loss:1.2754228732585906\n",
            "Epoch: 484,Batch : 1500, Loss:1.2754044221043588\n",
            "Epoch: 485,Batch : 1500, Loss:1.2749809653759003\n",
            "Epoch: 486,Batch : 1500, Loss:1.2741505586504935\n",
            "Epoch: 487,Batch : 1500, Loss:1.274951124727726\n",
            "Epoch: 488,Batch : 1500, Loss:1.2749438312649728\n",
            "Epoch: 489,Batch : 1500, Loss:1.2743757447004318\n",
            "Epoch: 490,Batch : 1500, Loss:1.2748869158029557\n",
            "Epoch: 491,Batch : 1500, Loss:1.2743156383633614\n",
            "testset Accuracy  : 24.5\n",
            "Epoch: 492,Batch : 1500, Loss:1.2748604055643082\n",
            "Epoch: 493,Batch : 1500, Loss:1.2747691124677658\n",
            "Epoch: 494,Batch : 1500, Loss:1.2735820427536964\n",
            "Epoch: 495,Batch : 1500, Loss:1.2739894097447395\n",
            "Epoch: 496,Batch : 1500, Loss:1.273635801255703\n",
            "Epoch: 497,Batch : 1500, Loss:1.273538712799549\n",
            "Epoch: 498,Batch : 1500, Loss:1.2743218250870705\n",
            "Epoch: 499,Batch : 1500, Loss:1.2743058295845986\n",
            "Epoch: 500,Batch : 1500, Loss:1.2735931624174117\n",
            "Epoch: 501,Batch : 1500, Loss:1.2731987463235854\n",
            "testset Accuracy  : 24.3\n",
            "Epoch: 502,Batch : 1500, Loss:1.2738285989761353\n",
            "Epoch: 503,Batch : 1500, Loss:1.2732174170017243\n",
            "Epoch: 504,Batch : 1500, Loss:1.2744649983644485\n",
            "Epoch: 505,Batch : 1500, Loss:1.2737366889119148\n",
            "Epoch: 506,Batch : 1500, Loss:1.2731615669727325\n",
            "Epoch: 507,Batch : 1500, Loss:1.274053457558155\n",
            "Epoch: 508,Batch : 1500, Loss:1.274795453429222\n",
            "Epoch: 509,Batch : 1500, Loss:1.2735994548201561\n",
            "Epoch: 510,Batch : 1500, Loss:1.2735631516575814\n",
            "Epoch: 511,Batch : 1500, Loss:1.2728491693139077\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 512,Batch : 1500, Loss:1.2722814221382142\n",
            "Epoch: 513,Batch : 1500, Loss:1.273824741899967\n",
            "Epoch: 514,Batch : 1500, Loss:1.2723570371270179\n",
            "Epoch: 515,Batch : 1500, Loss:1.2734530532956123\n",
            "Epoch: 516,Batch : 1500, Loss:1.2743072762489318\n",
            "Epoch: 517,Batch : 1500, Loss:1.272626030921936\n",
            "Epoch: 518,Batch : 1500, Loss:1.2727946169376374\n",
            "Epoch: 519,Batch : 1500, Loss:1.2738365322351455\n",
            "Epoch: 520,Batch : 1500, Loss:1.2720879064202308\n",
            "Epoch: 521,Batch : 1500, Loss:1.2733995950222015\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 522,Batch : 1500, Loss:1.2737207764983176\n",
            "Epoch: 523,Batch : 1500, Loss:1.274587297797203\n",
            "Epoch: 524,Batch : 1500, Loss:1.2732484450340271\n",
            "Epoch: 525,Batch : 1500, Loss:1.2737506098747253\n",
            "Epoch: 526,Batch : 1500, Loss:1.2738070725798607\n",
            "Epoch: 527,Batch : 1500, Loss:1.2725234741568565\n",
            "Epoch: 528,Batch : 1500, Loss:1.2716214810609818\n",
            "Epoch: 529,Batch : 1500, Loss:1.2730980194211006\n",
            "Epoch: 530,Batch : 1500, Loss:1.2727906723022462\n",
            "Epoch: 531,Batch : 1500, Loss:1.2725295025706291\n",
            "testset Accuracy  : 23.45\n",
            "Epoch: 532,Batch : 1500, Loss:1.2720650102496147\n",
            "Epoch: 533,Batch : 1500, Loss:1.273281185090542\n",
            "Epoch: 534,Batch : 1500, Loss:1.2730479789972304\n",
            "Epoch: 535,Batch : 1500, Loss:1.2721833974719048\n",
            "Epoch: 536,Batch : 1500, Loss:1.272987667441368\n",
            "Epoch: 537,Batch : 1500, Loss:1.2727409576773643\n",
            "Epoch: 538,Batch : 1500, Loss:1.2729311489462853\n",
            "Epoch: 539,Batch : 1500, Loss:1.2731819394230843\n",
            "Epoch: 540,Batch : 1500, Loss:1.273435612797737\n",
            "Epoch: 541,Batch : 1500, Loss:1.2728023384213447\n",
            "testset Accuracy  : 24.05\n",
            "Epoch: 542,Batch : 1500, Loss:1.2737500920295715\n",
            "Epoch: 543,Batch : 1500, Loss:1.2726512947678565\n",
            "Epoch: 544,Batch : 1500, Loss:1.2726710435152053\n",
            "Epoch: 545,Batch : 1500, Loss:1.2728434463143348\n",
            "Epoch: 546,Batch : 1500, Loss:1.2717573924660683\n",
            "Epoch: 547,Batch : 1500, Loss:1.2730734301805495\n",
            "Epoch: 548,Batch : 1500, Loss:1.272616789996624\n",
            "Epoch: 549,Batch : 1500, Loss:1.2728794957399368\n",
            "Epoch: 550,Batch : 1500, Loss:1.2720645709633827\n",
            "Epoch: 551,Batch : 1500, Loss:1.2723522570729255\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 552,Batch : 1500, Loss:1.2724177156686782\n",
            "Epoch: 553,Batch : 1500, Loss:1.2722974375486373\n",
            "Epoch: 554,Batch : 1500, Loss:1.27230319917202\n",
            "Epoch: 555,Batch : 1500, Loss:1.2726024235486983\n",
            "Epoch: 556,Batch : 1500, Loss:1.2733387038111688\n",
            "Epoch: 557,Batch : 1500, Loss:1.2721672127842902\n",
            "Epoch: 558,Batch : 1500, Loss:1.2721200470924376\n",
            "Epoch: 559,Batch : 1500, Loss:1.271930486023426\n",
            "Epoch: 560,Batch : 1500, Loss:1.2724153330922128\n",
            "Epoch: 561,Batch : 1500, Loss:1.2728040813207626\n",
            "testset Accuracy  : 24.3\n",
            "Epoch: 562,Batch : 1500, Loss:1.271939616203308\n",
            "Epoch: 563,Batch : 1500, Loss:1.272806067943573\n",
            "Epoch: 564,Batch : 1500, Loss:1.271736141562462\n",
            "Epoch: 565,Batch : 1500, Loss:1.2723007353544236\n",
            "Epoch: 566,Batch : 1500, Loss:1.272181313931942\n",
            "Epoch: 567,Batch : 1500, Loss:1.2715313443541527\n",
            "Epoch: 568,Batch : 1500, Loss:1.2721567332744599\n",
            "Epoch: 569,Batch : 1500, Loss:1.2729637260437012\n",
            "Epoch: 570,Batch : 1500, Loss:1.2727488443255424\n",
            "Epoch: 571,Batch : 1500, Loss:1.2711214159727096\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 572,Batch : 1500, Loss:1.2725161215662957\n",
            "Epoch: 573,Batch : 1500, Loss:1.2736218074560166\n",
            "Epoch: 574,Batch : 1500, Loss:1.2729393759965897\n",
            "Epoch: 575,Batch : 1500, Loss:1.272340529859066\n",
            "Epoch: 576,Batch : 1500, Loss:1.2717008464932442\n",
            "Epoch: 577,Batch : 1500, Loss:1.2714114015698432\n",
            "Epoch: 578,Batch : 1500, Loss:1.271145395040512\n",
            "Epoch: 579,Batch : 1500, Loss:1.2710244095921517\n",
            "Epoch: 580,Batch : 1500, Loss:1.2721348799467087\n",
            "Epoch: 581,Batch : 1500, Loss:1.2723984897136689\n",
            "testset Accuracy  : 24.3\n",
            "Epoch: 582,Batch : 1500, Loss:1.2724560880064963\n",
            "Epoch: 583,Batch : 1500, Loss:1.2712343059778213\n",
            "Epoch: 584,Batch : 1500, Loss:1.2721953583359717\n",
            "Epoch: 585,Batch : 1500, Loss:1.2710732735395431\n",
            "Epoch: 586,Batch : 1500, Loss:1.2724544025063516\n",
            "Epoch: 587,Batch : 1500, Loss:1.2711697143912315\n",
            "Epoch: 588,Batch : 1500, Loss:1.2717662101984024\n",
            "Epoch: 589,Batch : 1500, Loss:1.2724696093797683\n",
            "Epoch: 590,Batch : 1500, Loss:1.2719783914089202\n",
            "Epoch: 591,Batch : 1500, Loss:1.2720436853170396\n",
            "testset Accuracy  : 24.1\n",
            "Epoch: 592,Batch : 1500, Loss:1.271690612256527\n",
            "Epoch: 593,Batch : 1500, Loss:1.271291390299797\n",
            "Epoch: 594,Batch : 1500, Loss:1.2716219835877418\n",
            "Epoch: 595,Batch : 1500, Loss:1.2706744483113288\n",
            "Epoch: 596,Batch : 1500, Loss:1.2714999987483024\n",
            "Epoch: 597,Batch : 1500, Loss:1.271367107450962\n",
            "Epoch: 598,Batch : 1500, Loss:1.2719707062840462\n",
            "Epoch: 599,Batch : 1500, Loss:1.2711134749650954\n",
            "Epoch: 600,Batch : 1500, Loss:1.271828415632248\n",
            "Epoch: 601,Batch : 1500, Loss:1.2714100509285926\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 602,Batch : 1500, Loss:1.271217972934246\n",
            "Epoch: 603,Batch : 1500, Loss:1.272063156545162\n",
            "Epoch: 604,Batch : 1500, Loss:1.2712805019021034\n",
            "Epoch: 605,Batch : 1500, Loss:1.2703483654856682\n",
            "Epoch: 606,Batch : 1500, Loss:1.2705762841701507\n",
            "Epoch: 607,Batch : 1500, Loss:1.270248992085457\n",
            "Epoch: 608,Batch : 1500, Loss:1.2710663090348244\n",
            "Epoch: 609,Batch : 1500, Loss:1.2710664230585098\n",
            "Epoch: 610,Batch : 1500, Loss:1.2704483551979064\n",
            "Epoch: 611,Batch : 1500, Loss:1.271470699250698\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 612,Batch : 1500, Loss:1.2717522799372674\n",
            "Epoch: 613,Batch : 1500, Loss:1.271483092367649\n",
            "Epoch: 614,Batch : 1500, Loss:1.2698765523433686\n",
            "Epoch: 615,Batch : 1500, Loss:1.2714796175360679\n",
            "Epoch: 616,Batch : 1500, Loss:1.2721053681969643\n",
            "Epoch: 617,Batch : 1500, Loss:1.270488861143589\n",
            "Epoch: 618,Batch : 1500, Loss:1.271123393535614\n",
            "Epoch: 619,Batch : 1500, Loss:1.2712355080246924\n",
            "Epoch: 620,Batch : 1500, Loss:1.2714164606332778\n",
            "Epoch: 621,Batch : 1500, Loss:1.271822827100754\n",
            "testset Accuracy  : 23.9\n",
            "Epoch: 622,Batch : 1500, Loss:1.2713203529715538\n",
            "Epoch: 623,Batch : 1500, Loss:1.2710499941110611\n",
            "Epoch: 624,Batch : 1500, Loss:1.2695765508413315\n",
            "Epoch: 625,Batch : 1500, Loss:1.271204111158848\n",
            "Epoch: 626,Batch : 1500, Loss:1.2711214627623557\n",
            "Epoch: 627,Batch : 1500, Loss:1.2713878107070924\n",
            "Epoch: 628,Batch : 1500, Loss:1.270373618721962\n",
            "Epoch: 629,Batch : 1500, Loss:1.2709848304986955\n",
            "Epoch: 630,Batch : 1500, Loss:1.2700935426354407\n",
            "Epoch: 631,Batch : 1500, Loss:1.2700592430233955\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 632,Batch : 1500, Loss:1.2702196910381318\n",
            "Epoch: 633,Batch : 1500, Loss:1.2711860654354095\n",
            "Epoch: 634,Batch : 1500, Loss:1.2708428499102593\n",
            "Epoch: 635,Batch : 1500, Loss:1.2712612600922584\n",
            "Epoch: 636,Batch : 1500, Loss:1.2704054824709892\n",
            "Epoch: 637,Batch : 1500, Loss:1.269852830350399\n",
            "Epoch: 638,Batch : 1500, Loss:1.2703120126128196\n",
            "Epoch: 639,Batch : 1500, Loss:1.270309952378273\n",
            "Epoch: 640,Batch : 1500, Loss:1.271700399041176\n",
            "Epoch: 641,Batch : 1500, Loss:1.2708713192343712\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 642,Batch : 1500, Loss:1.2704299211502075\n",
            "Epoch: 643,Batch : 1500, Loss:1.2714264340996742\n",
            "Epoch: 644,Batch : 1500, Loss:1.269122582256794\n",
            "Epoch: 645,Batch : 1500, Loss:1.2708138725757598\n",
            "Epoch: 646,Batch : 1500, Loss:1.2699876882433891\n",
            "Epoch: 647,Batch : 1500, Loss:1.2716390606760979\n",
            "Epoch: 648,Batch : 1500, Loss:1.270219082593918\n",
            "Epoch: 649,Batch : 1500, Loss:1.2697900797128678\n",
            "Epoch: 650,Batch : 1500, Loss:1.269784297466278\n",
            "Epoch: 651,Batch : 1500, Loss:1.2714095079898835\n",
            "testset Accuracy  : 24.4\n",
            "Epoch: 652,Batch : 1500, Loss:1.2704037501215935\n",
            "Epoch: 653,Batch : 1500, Loss:1.2705383341908454\n",
            "Epoch: 654,Batch : 1500, Loss:1.2703935908675195\n",
            "Epoch: 655,Batch : 1500, Loss:1.2707320576906205\n",
            "Epoch: 656,Batch : 1500, Loss:1.2707742646932603\n",
            "Epoch: 657,Batch : 1500, Loss:1.269909334242344\n",
            "Epoch: 658,Batch : 1500, Loss:1.2701269397735595\n",
            "Epoch: 659,Batch : 1500, Loss:1.2695419972538948\n",
            "Epoch: 660,Batch : 1500, Loss:1.2695120210647584\n",
            "Epoch: 661,Batch : 1500, Loss:1.2699202031493186\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 662,Batch : 1500, Loss:1.2701128677725793\n",
            "Epoch: 663,Batch : 1500, Loss:1.2700091519355774\n",
            "Epoch: 664,Batch : 1500, Loss:1.2701280938982964\n",
            "Epoch: 665,Batch : 1500, Loss:1.2694894167780877\n",
            "Epoch: 666,Batch : 1500, Loss:1.268998269855976\n",
            "Epoch: 667,Batch : 1500, Loss:1.2694759305119514\n",
            "Epoch: 668,Batch : 1500, Loss:1.2702782546877862\n",
            "Epoch: 669,Batch : 1500, Loss:1.2698591182231902\n",
            "Epoch: 670,Batch : 1500, Loss:1.2704542227387428\n",
            "Epoch: 671,Batch : 1500, Loss:1.2705306440591813\n",
            "testset Accuracy  : 24.25\n",
            "Epoch: 672,Batch : 1500, Loss:1.2703368668556214\n",
            "Epoch: 673,Batch : 1500, Loss:1.2696019808650016\n",
            "Epoch: 674,Batch : 1500, Loss:1.268919646024704\n",
            "Epoch: 675,Batch : 1500, Loss:1.2704321995973586\n",
            "Epoch: 676,Batch : 1500, Loss:1.2689778010249138\n",
            "Epoch: 677,Batch : 1500, Loss:1.2688397470116615\n",
            "Epoch: 678,Batch : 1500, Loss:1.2704988071918488\n",
            "Epoch: 679,Batch : 1500, Loss:1.2708599458932877\n",
            "Epoch: 680,Batch : 1500, Loss:1.2700283175110818\n",
            "Epoch: 681,Batch : 1500, Loss:1.2703879330158234\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 682,Batch : 1500, Loss:1.2693418006300927\n",
            "Epoch: 683,Batch : 1500, Loss:1.2711184577941894\n",
            "Epoch: 684,Batch : 1500, Loss:1.269052130997181\n",
            "Epoch: 685,Batch : 1500, Loss:1.2693920946717263\n",
            "Epoch: 686,Batch : 1500, Loss:1.270458867430687\n",
            "Epoch: 687,Batch : 1500, Loss:1.2703918320536614\n",
            "Epoch: 688,Batch : 1500, Loss:1.2699091719388962\n",
            "Epoch: 689,Batch : 1500, Loss:1.2689794594049453\n",
            "Epoch: 690,Batch : 1500, Loss:1.2698141717910767\n",
            "Epoch: 691,Batch : 1500, Loss:1.2704733744859695\n",
            "testset Accuracy  : 23.6\n",
            "Epoch: 692,Batch : 1500, Loss:1.2695731512308122\n",
            "Epoch: 693,Batch : 1500, Loss:1.2700143250226974\n",
            "Epoch: 694,Batch : 1500, Loss:1.2702942923307419\n",
            "Epoch: 695,Batch : 1500, Loss:1.2685731759667396\n",
            "Epoch: 696,Batch : 1500, Loss:1.2697035475969314\n",
            "Epoch: 697,Batch : 1500, Loss:1.2693807225227356\n",
            "Epoch: 698,Batch : 1500, Loss:1.270538570344448\n",
            "Epoch: 699,Batch : 1500, Loss:1.2695828924775123\n",
            "Epoch: 700,Batch : 1500, Loss:1.269287510573864\n",
            "Epoch: 701,Batch : 1500, Loss:1.27094687718153\n",
            "testset Accuracy  : 24.2\n",
            "Epoch: 702,Batch : 1500, Loss:1.2695625813603402\n",
            "Epoch: 703,Batch : 1500, Loss:1.2699095788002015\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(150,1000):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1500 ==1499:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "  if epoch % 10 == 0:\n",
        "    with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs= net(images)\n",
        "        _, predicted = torch.max(outputs.data,1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(\"testset Accuracy  : {}\".format(100* correct/total))\n",
        "    total=0\n",
        "    correct=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDJoCltxkKf"
      },
      "source": [
        "pretrained 넣기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0fKwJo7P4iQ"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/share/trained_model/lenet5.pth'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VJnnqHYI-Bm"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ZGkY0NX5if"
      },
      "outputs": [],
      "source": [
        "classes = ('plane','automobile','bird','cat','deer',\n",
        "           'dog','frog','horse','ship','truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOaHVH52X5kA",
        "outputId": "721fb60d-6b72-45bd-9654-a06fdc4d9dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 14.285714285714286 %\n",
            "Accuracy of automobile : 16.666666666666668 %\n",
            "Accuracy of bird : 12.5 %\n",
            "Accuracy of cat : 4.166666666666667 %\n",
            "Accuracy of deer : 21.428571428571427 %\n",
            "Accuracy of dog : 8.333333333333334 %\n",
            "Accuracy of frog : 12.5 %\n",
            "Accuracy of horse : 4.166666666666667 %\n",
            "Accuracy of ship : 21.428571428571427 %\n",
            "Accuracy of truck : 20.833333333333332 %\n"
          ]
        }
      ],
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    c= (predicted == labels).squeeze()\n",
        "    for i in range(4):\n",
        "      label= labels[i]\n",
        "      class_correct[label]+= c[i].item()\n",
        "      class_total[label]+= 1\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Accuracy of {} : {} %\".format(classes[i],100* class_correct[i]/class_total[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLanzQjcfLzg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44HdYYfb5Eja"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"trainset Accuracy  : {}\".format(100* correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKUJChxdX5Wn"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8NlBkmgX5a4"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hfizy9LCavz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}