{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "jittering = transforms.ColorJitter(brightness = (0.1,0.4), contrast = (0.1,0.4), saturation = (0.1,0.4))\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "qVmN3_oJRcnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "  jittering\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform  = transforms.Compose([ transforms.ToTensor(),\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "wP1IQkIIRcna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015eb4c1-f717-4751-f781-715d243b2985",
        "id": "9NNY4GZmRcna"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ],
      "metadata": {
        "id": "rXDVrCOzRcnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "metadata": {
        "id": "JqW3P08vRcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "Xw0JLqaBRcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9ee058-8cb2-461d-ecc9-9237b12125da",
        "id": "pNMS0arBRcnc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.7920472210049629\n",
            "Epoch: 1,Batch : 1500, Loss:0.6843177570104599\n",
            "Epoch: 2,Batch : 750, Loss:0.5877342796325684\n",
            "Epoch: 2,Batch : 1500, Loss:0.5034955669939518\n",
            "Epoch: 3,Batch : 750, Loss:0.4098104162812233\n",
            "Epoch: 3,Batch : 1500, Loss:0.37009351898729803\n",
            "Epoch: 4,Batch : 750, Loss:0.30713091406971216\n",
            "Epoch: 4,Batch : 1500, Loss:0.29542321026325224\n",
            "Epoch: 5,Batch : 750, Loss:0.24367058895528315\n",
            "Epoch: 5,Batch : 1500, Loss:0.24070696695894003\n",
            "Epoch: 6,Batch : 750, Loss:0.19173629730939865\n",
            "Epoch: 6,Batch : 1500, Loss:0.19567097444459797\n",
            "Epoch: 7,Batch : 750, Loss:0.15012819900736213\n",
            "Epoch: 7,Batch : 1500, Loss:0.15796856589242816\n",
            "Epoch: 8,Batch : 750, Loss:0.11138143332116306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "l53QgYP0Rcnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "M-JQPoEbRcnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "xLY6gDwMRcnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "lTTZLf1-R2Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d48397-9130-4ce8-ba18-f9027512e78c",
        "id": "F-Uwjs7XR2Ds"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    seed = 42\n",
        "    device = \"cuda:0\"    \n",
        "        \n",
        "    lr = 1e-3\n",
        "    epochs = 25\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    train_5_folds = True\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "\n",
        "seed_everything(config.seed)"
      ],
      "metadata": {
        "id": "Y537CROxR2Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71c5562-65e6-48c9-a211-404e675fe957",
        "id": "qn_fOyhUR2Dt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jittering = transforms.ColorJitter(brightness = 0.3, contrast = 0.3, saturation = 0.4)\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "70_nXUACR2Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform  = transforms.Compose([ transforms.ToTensor(),\n",
        "                                                                       transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "        ])"
      ],
      "metadata": {
        "id": "YeDYgykUR2Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97efe0e8-890c-49a7-be91-7700ac768850",
        "id": "4z397PELR2Du"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ],
      "metadata": {
        "id": "Zd8-9_oDSQK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "metadata": {
        "id": "JRxWLJjnKUIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 넣기 "
      ],
      "metadata": {
        "id": "1hTgHG91xbSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "   def __init__(self, in_channels, out_channels, kernel_size=3):\n",
        "       super(BasicBlock, self).__init__()\n",
        "\n",
        "\n",
        "       # ❶ 합성곱층 정의\n",
        "       self.c1 = nn.Conv2d(in_channels, out_channels, \n",
        "                           kernel_size=kernel_size, padding=1)\n",
        "       self.c2 = nn.Conv2d(out_channels, out_channels, \n",
        "                           kernel_size=kernel_size, padding=1)\n",
        "\n",
        "       self.downsample = nn.Conv2d(in_channels, out_channels, \n",
        "                                   kernel_size=1)\n",
        "       \n",
        "       # ❷ 배치 정규화층 정의\n",
        "       self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
        "       self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "       self.relu = nn.ReLU()\n",
        "   def forward(self, x):\n",
        "       # ❸스킵 커넥션을 위해 초기 입력을 저장\n",
        "       x_ = x\n",
        "\n",
        "       x = self.c1(x)\n",
        "       x = self.bn1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.c2(x)\n",
        "       x = self.bn2(x)\n",
        "\n",
        "       # ➍합성곱의 결과와 입력의 채널 수를 맞춤\n",
        "       x_ = self.downsample(x_)\n",
        "\n",
        "       # ➎합성곱층의 결과와 저장해놨던 입력값을 더해줌\n",
        "       x += x_\n",
        "       x = self.relu(x)\n",
        "\n",
        "       return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "   def __init__(self, num_classes=10):\n",
        "       super(ResNet, self).__init__()\n",
        "\n",
        "\n",
        "       # ❶ 기본 블록\n",
        "       self.b1 = BasicBlock(in_channels=3, out_channels=64)\n",
        "       self.b2 = BasicBlock(in_channels=64, out_channels=128)\n",
        "       self.b3 = BasicBlock(in_channels=128, out_channels=256)\n",
        "\n",
        "\n",
        "       # ❷ 풀링을 최댓값이 아닌 평균값으로\n",
        "       self.pool = nn.AvgPool2d(kernel_size=2, stride=2) \n",
        "\n",
        "       # ❸ 분류기\n",
        "       self.fc1 = nn.Linear(in_features=4096, out_features=2048)\n",
        "       self.fc2 = nn.Linear(in_features=2048, out_features=512)\n",
        "       self.fc3 = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "       self.relu = nn.ReLU()\n",
        "   def forward(self, x):\n",
        "       # ❶ 기본 블록과 풀링층을 통과\n",
        "       x = self.b1(x)\n",
        "       x = self.pool(x)\n",
        "       x = self.b2(x)\n",
        "       x = self.pool(x)\n",
        "       x = self.b3(x)\n",
        "       x = self.pool(x)\n",
        "\n",
        "\n",
        "       # ❷ 분류기의 입력으로 사용하기 위해 flatten\n",
        "       x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "       # ❸ 분류기로 예측값 출력\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc3(x)\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "NkV4EfO4VplZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "Os2IBshCP2bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=0.000001)"
      ],
      "metadata": {
        "id": "BFPIEQ-pyDL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8IufYT0P4Zx",
        "outputId": "cf59b749-0339-40c1-c775-eb5a3af100fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.5604290823638439\n",
            "Epoch: 1,Batch : 1500, Loss:0.3965580953657627\n",
            "Epoch: 2,Batch : 750, Loss:0.29546111458539964\n",
            "Epoch: 2,Batch : 1500, Loss:0.258781103014946\n",
            "Epoch: 3,Batch : 750, Loss:0.19565488464385272\n",
            "Epoch: 3,Batch : 1500, Loss:0.1893154518455267\n",
            "Epoch: 4,Batch : 750, Loss:0.12729302999190986\n",
            "Epoch: 4,Batch : 1500, Loss:0.13379417736269533\n",
            "Epoch: 5,Batch : 750, Loss:0.08211038008052855\n",
            "Epoch: 5,Batch : 1500, Loss:0.09357821545749903\n",
            "Epoch: 6,Batch : 750, Loss:0.049738563649589194\n",
            "Epoch: 6,Batch : 1500, Loss:0.061254371469840405\n",
            "Epoch: 7,Batch : 750, Loss:0.039112058409664315\n",
            "Epoch: 7,Batch : 1500, Loss:0.04363377985497936\n",
            "Epoch: 8,Batch : 750, Loss:0.0303137074377737\n",
            "Epoch: 8,Batch : 1500, Loss:0.03550255238701357\n",
            "Epoch: 9,Batch : 750, Loss:0.026535421120759566\n",
            "Epoch: 9,Batch : 1500, Loss:0.03083486866115709\n",
            "Epoch: 10,Batch : 750, Loss:0.022291735005768715\n",
            "Epoch: 10,Batch : 1500, Loss:0.028727351570341852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "KqDJoCltxkKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaild Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44HdYYfb5Eja",
        "outputId": "7ff581ef-5cf4-4210-fb63-d7cb8e2ca77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainset Accuracy  : 81.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "O8NlBkmgX5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae64564-5c36-4b3f-d287-e0b2e5c1d2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset Accuracy  : 47.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "62Bjv6A6Mg2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jittering = transforms.ColorJitter(brightness = 0.3, contrast = 0.3, saturation = 0.4)\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "YwtDfMa8MdJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        " transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform  = transforms.Compose([ transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "        ])"
      ],
      "metadata": {
        "id": "8l5y9PZZMdJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e1ad13-9c3b-42cf-f6be-193c35965c33",
        "id": "tq2C1-XWMdJD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ],
      "metadata": {
        "id": "O1OCmKedMdJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "metadata": {
        "id": "8tegHqCMMdJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "bDoxf2QIMZks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc445545-79cc-424f-afaa-c5624714e680",
        "id": "xWMYhCNQMZkt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.5742686148583889\n",
            "Epoch: 1,Batch : 1500, Loss:0.4106494539231062\n",
            "Epoch: 2,Batch : 750, Loss:0.31436835338175295\n",
            "Epoch: 2,Batch : 1500, Loss:0.2706271687820554\n",
            "Epoch: 3,Batch : 750, Loss:0.20704827290028333\n",
            "Epoch: 3,Batch : 1500, Loss:0.19835794933885337\n",
            "Epoch: 4,Batch : 750, Loss:0.1424439019560814\n",
            "Epoch: 4,Batch : 1500, Loss:0.14086420067958533\n",
            "Epoch: 5,Batch : 750, Loss:0.08884132007323206\n",
            "Epoch: 5,Batch : 1500, Loss:0.09541876823827625\n",
            "Epoch: 6,Batch : 750, Loss:0.05447366884723306\n",
            "Epoch: 6,Batch : 1500, Loss:0.0655631344676949\n",
            "Epoch: 7,Batch : 750, Loss:0.03749638847797178\n",
            "Epoch: 7,Batch : 1500, Loss:0.04783218496257905\n",
            "Epoch: 8,Batch : 750, Loss:0.028435727234435035\n",
            "Epoch: 8,Batch : 1500, Loss:0.036793885451334066\n",
            "Epoch: 9,Batch : 750, Loss:0.023062976538902148\n",
            "Epoch: 9,Batch : 1500, Loss:0.03179990821040701\n",
            "Epoch: 10,Batch : 750, Loss:0.023008025035989702\n",
            "Epoch: 10,Batch : 1500, Loss:0.02794325759138883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "GBNT-E46MZku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "8U2DdYiDMZku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8bea66a-e742-4dec-859c-f6275bdfc267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vaildset Accuracy  : 81.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "id": "IjsdAnW2MZkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fc32f3-f923-4ad9-e90f-c01ee10cb715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset Accuracy  : 43.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3-9X9RwTP_86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jittering = transforms.ColorJitter(brightness = (0.1,0.4), contrast = (0.1,0.4), saturation = (0.1,0.4))\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "D_eIZHT2QAkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "  jittering\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform  = transforms.Compose([ transforms.ToTensor(),\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "b38lzyPxQAkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015eb4c1-f717-4751-f781-715d243b2985",
        "id": "7aGVEZnYQAkE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ],
      "metadata": {
        "id": "3gqClC_4QAkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "metadata": {
        "id": "UEJ0s7WrQAkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "k2uUORpAQAkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9ee058-8cb2-461d-ecc9-9237b12125da",
        "id": "av6aSZSpQAkG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.7920472210049629\n",
            "Epoch: 1,Batch : 1500, Loss:0.6843177570104599\n",
            "Epoch: 2,Batch : 750, Loss:0.5877342796325684\n",
            "Epoch: 2,Batch : 1500, Loss:0.5034955669939518\n",
            "Epoch: 3,Batch : 750, Loss:0.4098104162812233\n",
            "Epoch: 3,Batch : 1500, Loss:0.37009351898729803\n",
            "Epoch: 4,Batch : 750, Loss:0.30713091406971216\n",
            "Epoch: 4,Batch : 1500, Loss:0.29542321026325224\n",
            "Epoch: 5,Batch : 750, Loss:0.24367058895528315\n",
            "Epoch: 5,Batch : 1500, Loss:0.24070696695894003\n",
            "Epoch: 6,Batch : 750, Loss:0.19173629730939865\n",
            "Epoch: 6,Batch : 1500, Loss:0.19567097444459797\n",
            "Epoch: 7,Batch : 750, Loss:0.15012819900736213\n",
            "Epoch: 7,Batch : 1500, Loss:0.15796856589242816\n",
            "Epoch: 8,Batch : 750, Loss:0.11138143332116306\n",
            "Epoch: 8,Batch : 1500, Loss:0.12226440766081213\n",
            "Epoch: 9,Batch : 750, Loss:0.08254361521266401\n",
            "Epoch: 9,Batch : 1500, Loss:0.09834219278208911\n",
            "Epoch: 10,Batch : 750, Loss:0.06141183136124164\n",
            "Epoch: 10,Batch : 1500, Loss:0.07319813255453482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "etG6tCboQAkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a7e6c6-3864-4d79-bb92-d3212b9c1f90",
        "id": "BMXl2TFpQAkH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vaildset Accuracy  : 74.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501dfacc-26b0-4a4a-a0b4-149ad1863be9",
        "id": "K5ITcU35QAkH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset Accuracy  : 42.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MgBiLxeQRZ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jittering = transforms.ColorJitter(brightness = (0.1,0.4), contrast = (0.1,0.4), saturation = (0.1,0.4))\n",
        "\n",
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "FqJdVN8bRf4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform_train = transforms.Compose([transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(p=0.5)\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform  = transforms.Compose([ transforms.ToTensor(),\n",
        "\n",
        "        ])"
      ],
      "metadata": {
        "id": "Fdka_OvARf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "\n",
        "\n",
        "\n",
        "vaildset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/share/cafir_10',\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e3156f1-c1e0-4426-f4bc-27b38c771c37",
        "id": "0hqUzGG4Rf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.ImageFolder(root = \"/content/drive/MyDrive/share/Statistical_Deep_Image\",\n",
        "                                           transform = transform)"
      ],
      "metadata": {
        "id": "sbjgHNcqRf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trainset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "vaild_loader = DataLoader(vaildset,\n",
        "                          batch_size = 32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(testset,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)"
      ],
      "metadata": {
        "id": "cbDr2g89Rf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "total=0 \n",
        "correct = 0"
      ],
      "metadata": {
        "id": "pbY4rkKeRf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs= net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 750 ==749:\n",
        "      print(\"Epoch: {},Batch : {}, Loss:{}\".format(epoch+1, i+1, running_loss/2000))\n",
        "      running_loss = 0.0\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af254a4b-e8c4-4dcc-e6b4-18a25c1e94be",
        "id": "SF3Jw0H5Rf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,Batch : 750, Loss:0.5877426655590534\n",
            "Epoch: 1,Batch : 1500, Loss:0.4274163376092911\n",
            "Epoch: 2,Batch : 750, Loss:0.3346704117059708\n",
            "Epoch: 2,Batch : 1500, Loss:0.283121898651123\n",
            "Epoch: 3,Batch : 750, Loss:0.23469326613098382\n",
            "Epoch: 3,Batch : 1500, Loss:0.2157737657725811\n",
            "Epoch: 4,Batch : 750, Loss:0.17674002100899816\n",
            "Epoch: 4,Batch : 1500, Loss:0.175645044259727\n",
            "Epoch: 5,Batch : 750, Loss:0.14346168457344174\n",
            "Epoch: 5,Batch : 1500, Loss:0.14264629148319363\n",
            "Epoch: 6,Batch : 750, Loss:0.11532783246412873\n",
            "Epoch: 6,Batch : 1500, Loss:0.1194775180220604\n",
            "Epoch: 7,Batch : 750, Loss:0.09187927455734461\n",
            "Epoch: 7,Batch : 1500, Loss:0.09957177088037134\n",
            "Epoch: 8,Batch : 750, Loss:0.07512747709918767\n",
            "Epoch: 8,Batch : 1500, Loss:0.0822572408085689\n",
            "Epoch: 9,Batch : 750, Loss:0.06276898121333216\n",
            "Epoch: 9,Batch : 1500, Loss:0.07087478585727512\n",
            "Epoch: 10,Batch : 750, Loss:0.05445669889682904\n",
            "Epoch: 10,Batch : 1500, Loss:0.05837930965819396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pretrained 넣기\n"
      ],
      "metadata": {
        "id": "1TW5UNGZRf4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in vaild_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"vaildset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5kx0Dp2Rf4b",
        "outputId": "b156394d-dc0f-4621-bd00-6fec4f7f5b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vaildset Accuracy  : 84.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    outputs= net(images)\n",
        "    _, predicted = torch.max(outputs.data,1)\n",
        "    total +=labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"testset Accuracy  : {}\".format(100* correct/total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxGe0FTIRf4b",
        "outputId": "42dc9fe1-7747-432b-d3e7-85f9138b13c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset Accuracy  : 48.45\n"
          ]
        }
      ]
    }
  ]
}